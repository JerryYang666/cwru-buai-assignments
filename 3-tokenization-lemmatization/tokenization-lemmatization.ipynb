{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63880d86",
   "metadata": {},
   "source": [
    "BUAI 435 Assignment 3 - Tokenization & Lemmatization  \n",
    "Name: Ruihuang Yang  \n",
    "NetID: rxy216  \n",
    "Date: 10/01/2025  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a61eb",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908da550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde68f78",
   "metadata": {},
   "source": [
    "### Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2615a30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (904765, 15)\n",
      "Sampled dataset shape (1%): (9048, 15)\n",
      "\n",
      "Column names:\n",
      "['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date']\n",
      "\n",
      "First few rows of review_body:\n",
      "0    Needed this for my percussion class. Works great.\n",
      "1    Ive gone through my share of headphones, these...\n",
      "2    I recently purchased a very nice Fender Acoust...\n",
      "3    These drum sticks were almost what they descri...\n",
      "4    Great microphone. Works well with my Nikon set...\n",
      "Name: review_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the Amazon Musical dataset\n",
    "df = pd.read_csv('data/Amazon_Musical.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Sample 1% of the dataset for faster processing (especially for lemmatization in Q5)\n",
    "df = df.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
    "print(f\"Sampled dataset shape (1%): {df.shape}\")\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows of review_body:\")\n",
    "print(df['review_body'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050cfd12",
   "metadata": {},
   "source": [
    "## Q1 — Keras Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab35adfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q1 — Keras Tokenizer Results\n",
      "================================================================================\n",
      "\n",
      "--- Row 0 ---\n",
      "Original review_body:\n",
      "Needed this for my percussion class. Works great.\n",
      "\n",
      "Tokenized (token_keras):\n",
      "['needed', 'this', 'for', 'my', 'percussion', 'class', 'works', 'great']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 1 ---\n",
      "Original review_body:\n",
      "Ive gone through my share of headphones, these are better than spending $300 or more on dr. dre beats, these have great quality highs-low-bass ect. very comfortable over the ears and it even has lil swivels that you can rotate to sit straight round your neck with the ear parts laying flat on your torso. Fast shipping as well and would recommend getting these!\n",
      "\n",
      "Tokenized (token_keras):\n",
      "['ive', 'gone', 'through', 'my', 'share', 'of', 'headphones', 'these', 'are', 'better', 'than', 'spending', '300', 'or', 'more', 'on', 'dr', 'dre', 'beats', 'these', 'have', 'great', 'quality', 'highs', 'low', 'bass', 'ect', 'very', 'comfortable', 'over', 'the', 'ears', 'and', 'it', 'even', 'has', 'lil', 'swivels', 'that', 'you', 'can', 'rotate', 'to', 'sit', 'straight', 'round', 'your', 'neck', 'with', 'the', 'ear', 'parts', 'laying', 'flat', 'on', 'your', 'torso', 'fast', 'shipping', 'as', 'well', 'and', 'would', 'recommend', 'getting', 'these']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 2 ---\n",
      "Original review_body:\n",
      "I recently purchased a very nice Fender Acoustic Guitar (her name is &#34;Suzanne&#34;  -- I name all my guitars).  I purchased this case because it looked nice and the price was very, very nice.  I have owned more than a couple of guitar cases and this is easily the nicest and most beautiful case I have ever owned.  I could not be more satisfied -- no, delighted with this case and this purchase.  You will feel the same about yours once you buy one.  The quality of construction and the solid way it is put together causes me to believe this case will hold up for many years of use.\n",
      "\n",
      "Tokenized (token_keras):\n",
      "['i', 'recently', 'purchased', 'a', 'very', 'nice', 'fender', 'acoustic', 'guitar', 'her', 'name', 'is', '34', 'suzanne', '34', 'i', 'name', 'all', 'my', 'guitars', 'i', 'purchased', 'this', 'case', 'because', 'it', 'looked', 'nice', 'and', 'the', 'price', 'was', 'very', 'very', 'nice', 'i', 'have', 'owned', 'more', 'than', 'a', 'couple', 'of', 'guitar', 'cases', 'and', 'this', 'is', 'easily', 'the', 'nicest', 'and', 'most', 'beautiful', 'case', 'i', 'have', 'ever', 'owned', 'i', 'could', 'not', 'be', 'more', 'satisfied', 'no', 'delighted', 'with', 'this', 'case', 'and', 'this', 'purchase', 'you', 'will', 'feel', 'the', 'same', 'about', 'yours', 'once', 'you', 'buy', 'one', 'the', 'quality', 'of', 'construction', 'and', 'the', 'solid', 'way', 'it', 'is', 'put', 'together', 'causes', 'me', 'to', 'believe', 'this', 'case', 'will', 'hold', 'up', 'for', 'many', 'years', 'of', 'use']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 3 ---\n",
      "Original review_body:\n",
      "These drum sticks were almost what they described but with one IIMPORTANT difference... THEY WERE NOT NYLON TIPPED. As a drummer that makes a HUGE difference when playing.<br />  You may well ask why I never returned them. The answer is that I actually would almost lose money to do so. I would only get back the price of the sticks - $16.00. I lose the shipping costs (approximately $11.00) PLUS the return shipping - about the same - and cost of fuel, materials, time and aggravation just to go to the post office to actually accomplish the return shipping. Add in the time it would take to get my money returned to my card and you can see why I decided to sell the sticks to a friend for half their value. I made back more money that way.<br />  I realize it's not Amazon's fault per se, but you might want to let your seller know they are NOT checking their products for comparability with their description of them. I also received notification, via USBS tracking, that it had been delivered when it wasn't and the package got here about a week later. All in all not worth the time and money!<br />Bruce Way\n",
      "\n",
      "Tokenized (token_keras):\n",
      "['these', 'drum', 'sticks', 'were', 'almost', 'what', 'they', 'described', 'but', 'with', 'one', 'iimportant', 'difference', 'they', 'were', 'not', 'nylon', 'tipped', 'as', 'a', 'drummer', 'that', 'makes', 'a', 'huge', 'difference', 'when', 'playing', 'br', 'you', 'may', 'well', 'ask', 'why', 'i', 'never', 'returned', 'them', 'the', 'answer', 'is', 'that', 'i', 'actually', 'would', 'almost', 'lose', 'money', 'to', 'do', 'so', 'i', 'would', 'only', 'get', 'back', 'the', 'price', 'of', 'the', 'sticks', '16', '00', 'i', 'lose', 'the', 'shipping', 'costs', 'approximately', '11', '00', 'plus', 'the', 'return', 'shipping', 'about', 'the', 'same', 'and', 'cost', 'of', 'fuel', 'materials', 'time', 'and', 'aggravation', 'just', 'to', 'go', 'to', 'the', 'post', 'office', 'to', 'actually', 'accomplish', 'the', 'return', 'shipping', 'add', 'in', 'the', 'time', 'it', 'would', 'take', 'to', 'get', 'my', 'money', 'returned', 'to', 'my', 'card', 'and', 'you', 'can', 'see', 'why', 'i', 'decided', 'to', 'sell', 'the', 'sticks', 'to', 'a', 'friend', 'for', 'half', 'their', 'value', 'i', 'made', 'back', 'more', 'money', 'that', 'way', 'br', 'i', 'realize', \"it's\", 'not', \"amazon's\", 'fault', 'per', 'se', 'but', 'you', 'might', 'want', 'to', 'let', 'your', 'seller', 'know', 'they', 'are', 'not', 'checking', 'their', 'products', 'for', 'comparability', 'with', 'their', 'description', 'of', 'them', 'i', 'also', 'received', 'notification', 'via', 'usbs', 'tracking', 'that', 'it', 'had', 'been', 'delivered', 'when', 'it', \"wasn't\", 'and', 'the', 'package', 'got', 'here', 'about', 'a', 'week', 'later', 'all', 'in', 'all', 'not', 'worth', 'the', 'time', 'and', 'money', 'br', 'bruce', 'way']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 4 ---\n",
      "Original review_body:\n",
      "Great microphone. Works well with my Nikon set up.\n",
      "\n",
      "Tokenized (token_keras):\n",
      "['great', 'microphone', 'works', 'well', 'with', 'my', 'nikon', 'set', 'up']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert review_body to string type to avoid errors with non-string values\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Apply text_to_word_sequence to each row of review_body\n",
    "df['token_keras'] = df['review_body'].apply(text_to_word_sequence)\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q1 — Keras Tokenizer Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_keras):\\n{df['token_keras'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31576c4f",
   "metadata": {},
   "source": [
    "## Q2 — Regex Tokenizer Version 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402ae9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q2 — Regex Tokenizer Version 1 Results\n",
      "================================================================================\n",
      "\n",
      "--- Row 0 ---\n",
      "Original review_body:\n",
      "Needed this for my percussion class. Works great.\n",
      "\n",
      "Tokenized (token_regex_ver1):\n",
      "['Needed', 'this', 'for', 'my', 'percussion', 'class', 'Works', 'great']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 1 ---\n",
      "Original review_body:\n",
      "Ive gone through my share of headphones, these are better than spending $300 or more on dr. dre beats, these have great quality highs-low-bass ect. very comfortable over the ears and it even has lil swivels that you can rotate to sit straight round your neck with the ear parts laying flat on your torso. Fast shipping as well and would recommend getting these!\n",
      "\n",
      "Tokenized (token_regex_ver1):\n",
      "['Ive', 'gone', 'through', 'my', 'share', 'of', 'headphones', 'these', 'are', 'better', 'than', 'spending', 'or', 'more', 'on', 'dr', 'dre', 'beats', 'these', 'have', 'great', 'quality', 'highs', 'low', 'bass', 'ect', 'very', 'comfortable', 'over', 'the', 'ears', 'and', 'it', 'even', 'has', 'lil', 'swivels', 'that', 'you', 'can', 'rotate', 'to', 'sit', 'straight', 'round', 'your', 'neck', 'with', 'the', 'ear', 'parts', 'laying', 'flat', 'on', 'your', 'torso', 'Fast', 'shipping', 'as', 'well', 'and', 'would', 'recommend', 'getting', 'these']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 2 ---\n",
      "Original review_body:\n",
      "I recently purchased a very nice Fender Acoustic Guitar (her name is &#34;Suzanne&#34;  -- I name all my guitars).  I purchased this case because it looked nice and the price was very, very nice.  I have owned more than a couple of guitar cases and this is easily the nicest and most beautiful case I have ever owned.  I could not be more satisfied -- no, delighted with this case and this purchase.  You will feel the same about yours once you buy one.  The quality of construction and the solid way it is put together causes me to believe this case will hold up for many years of use.\n",
      "\n",
      "Tokenized (token_regex_ver1):\n",
      "['I', 'recently', 'purchased', 'a', 'very', 'nice', 'Fender', 'Acoustic', 'Guitar', 'her', 'name', 'is', 'Suzanne', 'I', 'name', 'all', 'my', 'guitars', 'I', 'purchased', 'this', 'case', 'because', 'it', 'looked', 'nice', 'and', 'the', 'price', 'was', 'very', 'very', 'nice', 'I', 'have', 'owned', 'more', 'than', 'a', 'couple', 'of', 'guitar', 'cases', 'and', 'this', 'is', 'easily', 'the', 'nicest', 'and', 'most', 'beautiful', 'case', 'I', 'have', 'ever', 'owned', 'I', 'could', 'not', 'be', 'more', 'satisfied', 'no', 'delighted', 'with', 'this', 'case', 'and', 'this', 'purchase', 'You', 'will', 'feel', 'the', 'same', 'about', 'yours', 'once', 'you', 'buy', 'one', 'The', 'quality', 'of', 'construction', 'and', 'the', 'solid', 'way', 'it', 'is', 'put', 'together', 'causes', 'me', 'to', 'believe', 'this', 'case', 'will', 'hold', 'up', 'for', 'many', 'years', 'of', 'use']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 3 ---\n",
      "Original review_body:\n",
      "These drum sticks were almost what they described but with one IIMPORTANT difference... THEY WERE NOT NYLON TIPPED. As a drummer that makes a HUGE difference when playing.<br />  You may well ask why I never returned them. The answer is that I actually would almost lose money to do so. I would only get back the price of the sticks - $16.00. I lose the shipping costs (approximately $11.00) PLUS the return shipping - about the same - and cost of fuel, materials, time and aggravation just to go to the post office to actually accomplish the return shipping. Add in the time it would take to get my money returned to my card and you can see why I decided to sell the sticks to a friend for half their value. I made back more money that way.<br />  I realize it's not Amazon's fault per se, but you might want to let your seller know they are NOT checking their products for comparability with their description of them. I also received notification, via USBS tracking, that it had been delivered when it wasn't and the package got here about a week later. All in all not worth the time and money!<br />Bruce Way\n",
      "\n",
      "Tokenized (token_regex_ver1):\n",
      "['These', 'drum', 'sticks', 'were', 'almost', 'what', 'they', 'described', 'but', 'with', 'one', 'IIMPORTANT', 'difference', 'THEY', 'WERE', 'NOT', 'NYLON', 'TIPPED', 'As', 'a', 'drummer', 'that', 'makes', 'a', 'HUGE', 'difference', 'when', 'playing', 'br', 'You', 'may', 'well', 'ask', 'why', 'I', 'never', 'returned', 'them', 'The', 'answer', 'is', 'that', 'I', 'actually', 'would', 'almost', 'lose', 'money', 'to', 'do', 'so', 'I', 'would', 'only', 'get', 'back', 'the', 'price', 'of', 'the', 'sticks', 'I', 'lose', 'the', 'shipping', 'costs', 'approximately', 'PLUS', 'the', 'return', 'shipping', 'about', 'the', 'same', 'and', 'cost', 'of', 'fuel', 'materials', 'time', 'and', 'aggravation', 'just', 'to', 'go', 'to', 'the', 'post', 'office', 'to', 'actually', 'accomplish', 'the', 'return', 'shipping', 'Add', 'in', 'the', 'time', 'it', 'would', 'take', 'to', 'get', 'my', 'money', 'returned', 'to', 'my', 'card', 'and', 'you', 'can', 'see', 'why', 'I', 'decided', 'to', 'sell', 'the', 'sticks', 'to', 'a', 'friend', 'for', 'half', 'their', 'value', 'I', 'made', 'back', 'more', 'money', 'that', 'way', 'br', 'I', 'realize', 'it', 's', 'not', 'Amazon', 's', 'fault', 'per', 'se', 'but', 'you', 'might', 'want', 'to', 'let', 'your', 'seller', 'know', 'they', 'are', 'NOT', 'checking', 'their', 'products', 'for', 'comparability', 'with', 'their', 'description', 'of', 'them', 'I', 'also', 'received', 'notification', 'via', 'USBS', 'tracking', 'that', 'it', 'had', 'been', 'delivered', 'when', 'it', 'wasn', 't', 'and', 'the', 'package', 'got', 'here', 'about', 'a', 'week', 'later', 'All', 'in', 'all', 'not', 'worth', 'the', 'time', 'and', 'money', 'br', 'Bruce', 'Way']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 4 ---\n",
      "Original review_body:\n",
      "Great microphone. Works well with my Nikon set up.\n",
      "\n",
      "Tokenized (token_regex_ver1):\n",
      "['Great', 'microphone', 'Works', 'well', 'with', 'my', 'Nikon', 'set', 'up']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define compiled regex pattern to match one or more English letters\n",
    "pattern = re.compile(r\"[A-Za-z]+\")\n",
    "\n",
    "# Ensure review_body is treated as string\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Apply pattern.findall(x) to each row of review_body\n",
    "df['token_regex_ver1'] = df['review_body'].apply(lambda x: pattern.findall(x))\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q2 — Regex Tokenizer Version 1 Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_regex_ver1):\\n{df['token_regex_ver1'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2eed39",
   "metadata": {},
   "source": [
    "## Q3 — Regex Tokenizer Version 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753508c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q3 — Regex Tokenizer Version 2 Results\n",
      "================================================================================\n",
      "\n",
      "--- Row 0 ---\n",
      "Original review_body:\n",
      "Needed this for my percussion class. Works great.\n",
      "\n",
      "Tokenized (token_regex_ver2):\n",
      "['Needed', 'this', 'for', 'my', 'percussion', 'class', 'Works', 'great']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 1 ---\n",
      "Original review_body:\n",
      "Ive gone through my share of headphones, these are better than spending $300 or more on dr. dre beats, these have great quality highs-low-bass ect. very comfortable over the ears and it even has lil swivels that you can rotate to sit straight round your neck with the ear parts laying flat on your torso. Fast shipping as well and would recommend getting these!\n",
      "\n",
      "Tokenized (token_regex_ver2):\n",
      "['Ive', 'gone', 'through', 'my', 'share', 'of', 'headphones', 'these', 'are', 'better', 'than', 'spending', '300', 'or', 'more', 'on', 'dr', 'dre', 'beats', 'these', 'have', 'great', 'quality', 'highs', 'low', 'bass', 'ect', 'very', 'comfortable', 'over', 'the', 'ears', 'and', 'it', 'even', 'has', 'lil', 'swivels', 'that', 'you', 'can', 'rotate', 'to', 'sit', 'straight', 'round', 'your', 'neck', 'with', 'the', 'ear', 'parts', 'laying', 'flat', 'on', 'your', 'torso', 'Fast', 'shipping', 'as', 'well', 'and', 'would', 'recommend', 'getting', 'these']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 2 ---\n",
      "Original review_body:\n",
      "I recently purchased a very nice Fender Acoustic Guitar (her name is &#34;Suzanne&#34;  -- I name all my guitars).  I purchased this case because it looked nice and the price was very, very nice.  I have owned more than a couple of guitar cases and this is easily the nicest and most beautiful case I have ever owned.  I could not be more satisfied -- no, delighted with this case and this purchase.  You will feel the same about yours once you buy one.  The quality of construction and the solid way it is put together causes me to believe this case will hold up for many years of use.\n",
      "\n",
      "Tokenized (token_regex_ver2):\n",
      "['I', 'recently', 'purchased', 'a', 'very', 'nice', 'Fender', 'Acoustic', 'Guitar', 'her', 'name', 'is', '34', 'Suzanne', '34', 'I', 'name', 'all', 'my', 'guitars', 'I', 'purchased', 'this', 'case', 'because', 'it', 'looked', 'nice', 'and', 'the', 'price', 'was', 'very', 'very', 'nice', 'I', 'have', 'owned', 'more', 'than', 'a', 'couple', 'of', 'guitar', 'cases', 'and', 'this', 'is', 'easily', 'the', 'nicest', 'and', 'most', 'beautiful', 'case', 'I', 'have', 'ever', 'owned', 'I', 'could', 'not', 'be', 'more', 'satisfied', 'no', 'delighted', 'with', 'this', 'case', 'and', 'this', 'purchase', 'You', 'will', 'feel', 'the', 'same', 'about', 'yours', 'once', 'you', 'buy', 'one', 'The', 'quality', 'of', 'construction', 'and', 'the', 'solid', 'way', 'it', 'is', 'put', 'together', 'causes', 'me', 'to', 'believe', 'this', 'case', 'will', 'hold', 'up', 'for', 'many', 'years', 'of', 'use']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 3 ---\n",
      "Original review_body:\n",
      "These drum sticks were almost what they described but with one IIMPORTANT difference... THEY WERE NOT NYLON TIPPED. As a drummer that makes a HUGE difference when playing.<br />  You may well ask why I never returned them. The answer is that I actually would almost lose money to do so. I would only get back the price of the sticks - $16.00. I lose the shipping costs (approximately $11.00) PLUS the return shipping - about the same - and cost of fuel, materials, time and aggravation just to go to the post office to actually accomplish the return shipping. Add in the time it would take to get my money returned to my card and you can see why I decided to sell the sticks to a friend for half their value. I made back more money that way.<br />  I realize it's not Amazon's fault per se, but you might want to let your seller know they are NOT checking their products for comparability with their description of them. I also received notification, via USBS tracking, that it had been delivered when it wasn't and the package got here about a week later. All in all not worth the time and money!<br />Bruce Way\n",
      "\n",
      "Tokenized (token_regex_ver2):\n",
      "['These', 'drum', 'sticks', 'were', 'almost', 'what', 'they', 'described', 'but', 'with', 'one', 'IIMPORTANT', 'difference', 'THEY', 'WERE', 'NOT', 'NYLON', 'TIPPED', 'As', 'a', 'drummer', 'that', 'makes', 'a', 'HUGE', 'difference', 'when', 'playing', 'br', 'You', 'may', 'well', 'ask', 'why', 'I', 'never', 'returned', 'them', 'The', 'answer', 'is', 'that', 'I', 'actually', 'would', 'almost', 'lose', 'money', 'to', 'do', 'so', 'I', 'would', 'only', 'get', 'back', 'the', 'price', 'of', 'the', 'sticks', '16', '00', 'I', 'lose', 'the', 'shipping', 'costs', 'approximately', '11', '00', 'PLUS', 'the', 'return', 'shipping', 'about', 'the', 'same', 'and', 'cost', 'of', 'fuel', 'materials', 'time', 'and', 'aggravation', 'just', 'to', 'go', 'to', 'the', 'post', 'office', 'to', 'actually', 'accomplish', 'the', 'return', 'shipping', 'Add', 'in', 'the', 'time', 'it', 'would', 'take', 'to', 'get', 'my', 'money', 'returned', 'to', 'my', 'card', 'and', 'you', 'can', 'see', 'why', 'I', 'decided', 'to', 'sell', 'the', 'sticks', 'to', 'a', 'friend', 'for', 'half', 'their', 'value', 'I', 'made', 'back', 'more', 'money', 'that', 'way', 'br', 'I', 'realize', 'it', 's', 'not', 'Amazon', 's', 'fault', 'per', 'se', 'but', 'you', 'might', 'want', 'to', 'let', 'your', 'seller', 'know', 'they', 'are', 'NOT', 'checking', 'their', 'products', 'for', 'comparability', 'with', 'their', 'description', 'of', 'them', 'I', 'also', 'received', 'notification', 'via', 'USBS', 'tracking', 'that', 'it', 'had', 'been', 'delivered', 'when', 'it', 'wasn', 't', 'and', 'the', 'package', 'got', 'here', 'about', 'a', 'week', 'later', 'All', 'in', 'all', 'not', 'worth', 'the', 'time', 'and', 'money', 'br', 'Bruce', 'Way']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 4 ---\n",
      "Original review_body:\n",
      "Great microphone. Works well with my Nikon set up.\n",
      "\n",
      "Tokenized (token_regex_ver2):\n",
      "['Great', 'microphone', 'Works', 'well', 'with', 'my', 'Nikon', 'set', 'up']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define compiled regex pattern to match one or more word characters\n",
    "pattern2 = re.compile(r\"\\w+\")\n",
    "\n",
    "# Ensure review_body is treated as string\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Apply pattern2.findall(x) to each row of review_body\n",
    "df['token_regex_ver2'] = df['review_body'].apply(lambda x: pattern2.findall(x))\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q3 — Regex Tokenizer Version 2 Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_regex_ver2):\\n{df['token_regex_ver2'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28021bd",
   "metadata": {},
   "source": [
    "## Q4 — Regex Tokenizer Version 3 & Removing Some Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f7cc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q4 — Regex Tokenizer Version 3 & Removing Stop Words Results\n",
      "================================================================================\n",
      "\n",
      "--- Row 0 ---\n",
      "Original review_body:\n",
      "Needed this for my percussion class. Works great.\n",
      "\n",
      "Tokenized (token_regex_ver3):\n",
      "['needed', 'this', 'my', 'percussion', 'class', 'works', 'great']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 1 ---\n",
      "Original review_body:\n",
      "Ive gone through my share of headphones, these are better than spending $300 or more on dr. dre beats, these have great quality highs-low-bass ect. very comfortable over the ears and it even has lil swivels that you can rotate to sit straight round your neck with the ear parts laying flat on your torso. Fast shipping as well and would recommend getting these!\n",
      "\n",
      "Tokenized (token_regex_ver3):\n",
      "['ive', 'gone', 'through', 'my', 'share', 'headphones', 'these', 'are', 'better', 'than', 'spending', 'more', 'dr', 'dre', 'beats', 'these', 'have', 'great', 'quality', 'highs', 'low', 'bass', 'ect', 'very', 'comfortable', 'over', 'ears', 'it', 'even', 'has', 'lil', 'swivels', 'that', 'you', 'can', 'rotate', 'sit', 'straight', 'round', 'your', 'neck', 'with', 'ear', 'parts', 'laying', 'flat', 'your', 'torso', 'fast', 'shipping', 'as', 'well', 'would', 'recommend', 'getting', 'these']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 2 ---\n",
      "Original review_body:\n",
      "I recently purchased a very nice Fender Acoustic Guitar (her name is &#34;Suzanne&#34;  -- I name all my guitars).  I purchased this case because it looked nice and the price was very, very nice.  I have owned more than a couple of guitar cases and this is easily the nicest and most beautiful case I have ever owned.  I could not be more satisfied -- no, delighted with this case and this purchase.  You will feel the same about yours once you buy one.  The quality of construction and the solid way it is put together causes me to believe this case will hold up for many years of use.\n",
      "\n",
      "Tokenized (token_regex_ver3):\n",
      "['i', 'recently', 'purchased', 'very', 'nice', 'fender', 'acoustic', 'guitar', 'her', 'name', 'is', 'suzanne', 'i', 'name', 'all', 'my', 'guitars', 'i', 'purchased', 'this', 'case', 'because', 'it', 'looked', 'nice', 'price', 'was', 'very', 'very', 'nice', 'i', 'have', 'owned', 'more', 'than', 'couple', 'guitar', 'cases', 'this', 'is', 'easily', 'nicest', 'most', 'beautiful', 'case', 'i', 'have', 'ever', 'owned', 'i', 'could', 'not', 'be', 'more', 'satisfied', 'no', 'delighted', 'with', 'this', 'case', 'this', 'purchase', 'you', 'will', 'feel', 'same', 'about', 'yours', 'once', 'you', 'buy', 'one', 'quality', 'construction', 'solid', 'way', 'it', 'is', 'put', 'together', 'causes', 'me', 'believe', 'this', 'case', 'will', 'hold', 'up', 'many', 'years', 'use']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 3 ---\n",
      "Original review_body:\n",
      "These drum sticks were almost what they described but with one IIMPORTANT difference... THEY WERE NOT NYLON TIPPED. As a drummer that makes a HUGE difference when playing.<br />  You may well ask why I never returned them. The answer is that I actually would almost lose money to do so. I would only get back the price of the sticks - $16.00. I lose the shipping costs (approximately $11.00) PLUS the return shipping - about the same - and cost of fuel, materials, time and aggravation just to go to the post office to actually accomplish the return shipping. Add in the time it would take to get my money returned to my card and you can see why I decided to sell the sticks to a friend for half their value. I made back more money that way.<br />  I realize it's not Amazon's fault per se, but you might want to let your seller know they are NOT checking their products for comparability with their description of them. I also received notification, via USBS tracking, that it had been delivered when it wasn't and the package got here about a week later. All in all not worth the time and money!<br />Bruce Way\n",
      "\n",
      "Tokenized (token_regex_ver3):\n",
      "['these', 'drum', 'sticks', 'were', 'almost', 'what', 'they', 'described', 'but', 'with', 'one', 'iimportant', 'difference', 'they', 'were', 'not', 'nylon', 'tipped', 'as', 'drummer', 'that', 'makes', 'huge', 'difference', 'when', 'playing', 'you', 'may', 'well', 'ask', 'why', 'i', 'never', 'returned', 'them', 'answer', 'is', 'that', 'i', 'actually', 'would', 'almost', 'lose', 'money', 'do', 'so', 'i', 'would', 'only', 'get', 'back', 'price', 'sticks', 'i', 'lose', 'shipping', 'costs', 'approximately', 'plus', 'return', 'shipping', 'about', 'same', 'cost', 'fuel', 'materials', 'time', 'aggravation', 'just', 'go', 'post', 'office', 'actually', 'accomplish', 'return', 'shipping', 'add', 'time', 'it', 'would', 'take', 'get', 'my', 'money', 'returned', 'my', 'card', 'you', 'can', 'see', 'why', 'i', 'decided', 'sell', 'sticks', 'friend', 'half', 'their', 'value', 'i', 'made', 'back', 'more', 'money', 'that', 'way', 'i', 'realize', \"it's\", 'not', \"amazon's\", 'fault', 'per', 'se', 'but', 'you', 'might', 'want', 'let', 'your', 'seller', 'know', 'they', 'are', 'not', 'checking', 'their', 'products', 'comparability', 'with', 'their', 'description', 'them', 'i', 'also', 'received', 'notification', 'via', 'usbs', 'tracking', 'that', 'it', 'had', 'been', 'delivered', 'when', 'it', \"wasn't\", 'package', 'got', 'here', 'about', 'week', 'later', 'all', 'all', 'not', 'worth', 'time', 'money', 'bruce', 'way']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 4 ---\n",
      "Original review_body:\n",
      "Great microphone. Works well with my Nikon set up.\n",
      "\n",
      "Tokenized (token_regex_ver3):\n",
      "['great', 'microphone', 'works', 'well', 'with', 'my', 'nikon', 'set', 'up']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define stoplist as a Python set\n",
    "STOPLIST = {\"the\", \"a\", \"an\", \"and\", \"or\", \"to\", \"of\", \"in\", \"for\", \"on\", \"br\"}\n",
    "\n",
    "# Define compiled regex pattern to match letters, underscores, or apostrophes\n",
    "pattern3 = re.compile(r\"[A-Za-z_']+\")\n",
    "\n",
    "# Ensure review_body is treated as string and convert to lowercase\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Extract tokens and remove stopwords\n",
    "df['token_regex_ver3'] = df['review_body'].str.lower().apply(\n",
    "    lambda x: [w for w in pattern3.findall(x) if w not in STOPLIST]\n",
    ")\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q4 — Regex Tokenizer Version 3 & Removing Stop Words Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_regex_ver3):\\n{df['token_regex_ver3'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb90947",
   "metadata": {},
   "source": [
    "## Q5 — Lemmatizer for Regex Version 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a4f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yangrh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q5 — Lemmatizer for Regex Version 2 Results\n",
      "================================================================================\n",
      "\n",
      "--- Row 0 ---\n",
      "Original review_body:\n",
      "Needed this for my percussion class. Works great.\n",
      "\n",
      "Lemmatized (lemmas):\n",
      "['need', 'percussion', 'class', 'work', 'great']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 1 ---\n",
      "Original review_body:\n",
      "Ive gone through my share of headphones, these are better than spending $300 or more on dr. dre beats, these have great quality highs-low-bass ect. very comfortable over the ears and it even has lil swivels that you can rotate to sit straight round your neck with the ear parts laying flat on your torso. Fast shipping as well and would recommend getting these!\n",
      "\n",
      "Lemmatized (lemmas):\n",
      "['go', 'share', 'headphone', 'well', 'spend', 'dr', 'dre', 'beat', 'great', 'quality', 'high', 'low', 'bass', 'ect', 'comfortable', 'ear', 'even', 'lil', 'swivel', 'rotate', 'sit', 'straight', 'round', 'neck', 'ear', 'part', 'lay', 'flat', 'torso', 'fast', 'shipping', 'well', 'would', 'recommend', 'get']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 2 ---\n",
      "Original review_body:\n",
      "I recently purchased a very nice Fender Acoustic Guitar (her name is &#34;Suzanne&#34;  -- I name all my guitars).  I purchased this case because it looked nice and the price was very, very nice.  I have owned more than a couple of guitar cases and this is easily the nicest and most beautiful case I have ever owned.  I could not be more satisfied -- no, delighted with this case and this purchase.  You will feel the same about yours once you buy one.  The quality of construction and the solid way it is put together causes me to believe this case will hold up for many years of use.\n",
      "\n",
      "Lemmatized (lemmas):\n",
      "['recently', 'purchase', 'nice', 'fender', 'acoustic', 'guitar', 'name', 'suzanne', 'name', 'guitar', 'purchase', 'case', 'look', 'nice', 'price', 'nice', 'couple', 'guitar', 'case', 'easily', 'nice', 'beautiful', 'case', 'ever', 'could', 'satisfied', 'delighted', 'case', 'purchase', 'feel', 'buy', 'one', 'quality', 'construction', 'solid', 'way', 'put', 'together', 'cause', 'believe', 'case', 'hold', 'many', 'year', 'use']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 3 ---\n",
      "Original review_body:\n",
      "These drum sticks were almost what they described but with one IIMPORTANT difference... THEY WERE NOT NYLON TIPPED. As a drummer that makes a HUGE difference when playing.<br />  You may well ask why I never returned them. The answer is that I actually would almost lose money to do so. I would only get back the price of the sticks - $16.00. I lose the shipping costs (approximately $11.00) PLUS the return shipping - about the same - and cost of fuel, materials, time and aggravation just to go to the post office to actually accomplish the return shipping. Add in the time it would take to get my money returned to my card and you can see why I decided to sell the sticks to a friend for half their value. I made back more money that way.<br />  I realize it's not Amazon's fault per se, but you might want to let your seller know they are NOT checking their products for comparability with their description of them. I also received notification, via USBS tracking, that it had been delivered when it wasn't and the package got here about a week later. All in all not worth the time and money!<br />Bruce Way\n",
      "\n",
      "Lemmatized (lemmas):\n",
      "['drum', 'stick', 'almost', 'describe', 'one', 'iimportant', 'difference', 'nylon', 'tipped', 'drummer', 'make', 'huge', 'difference', 'play', 'br', 'may', 'well', 'ask', 'never', 'return', 'answer', 'actually', 'would', 'almost', 'lose', 'money', 'would', 'get', 'back', 'price', 'stick', 'lose', 'shipping', 'cost', 'approximately', 'plus', 'return', 'ship', 'cost', 'fuel', 'material', 'time', 'aggravation', 'go', 'post', 'office', 'actually', 'accomplish', 'return', 'ship', 'add', 'time', 'would', 'take', 'get', 'money', 'return', 'card', 'see', 'decide', 'sell', 'stick', 'friend', 'half', 'value', 'make', 'back', 'money', 'way', 'br', 'realize', 'amazon', 'fault', 'per', 'se', 'might', 'want', 'let', 'seller', 'know', 'check', 'product', 'comparability', 'description', 'also', 'receive', 'notification', 'via', 'usbs', 'track', 'deliver', 'package', 'get', 'week', 'later', 'worth', 'time', 'money', 'br', 'bruce', 'way']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 4 ---\n",
      "Original review_body:\n",
      "Great microphone. Works well with my Nikon set up.\n",
      "\n",
      "Lemmatized (lemmas):\n",
      "['great', 'microphone', 'works', 'well', 'nikon', 'set']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English stopwords from NLTK into a Python set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define lemmatize_tokens function\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens as input and returns lemmatized tokens\n",
    "    with stopwords removed and only alphabetic tokens kept.\n",
    "    \"\"\"\n",
    "    # Join tokens into a string for spaCy to process\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # Create a spaCy doc object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract lemmas, convert to lowercase, keep only alphabetic tokens, remove stopwords\n",
    "    lemmas = [\n",
    "        token.lemma_.lower() \n",
    "        for token in doc \n",
    "        if token.is_alpha and token.lemma_.lower() not in stop_words\n",
    "    ]\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "# Apply lemmatize_tokens to token_regex_ver2 to produce lemmas column\n",
    "df['lemmas'] = df['token_regex_ver2'].apply(lemmatize_tokens)\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q5 — Lemmatizer for Regex Version 2 Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nLemmatized (lemmas):\\n{df['lemmas'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a1b25",
   "metadata": {},
   "source": [
    "## Q6 — Suggestion of Your Own Tokenizer\n",
    "\n",
    "**My Approach: Sentiment-Aware Lemmatization Pipeline for Amazon Reviews**\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "For Amazon review analysis, I propose a comprehensive tokenization pipeline that combines:\n",
    "1. **Regex tokenization with contraction handling** (`[A-Za-z']+` pattern)\n",
    "2. **Lowercase normalization** for consistency\n",
    "3. **Lemmatization** to reduce words to base forms\n",
    "4. **Sentiment-aware stopword removal** - preserves critical sentiment modifiers\n",
    "5. **Minimum token length filtering** (≥2 characters) to remove noise\n",
    "6. **Alphabetic-only filtering** to remove remaining noise\n",
    "\n",
    "### Why This Approach is Appropriate for Amazon Reviews:\n",
    "\n",
    "1. **Sentiment Analysis Ready**: Lemmatization normalizes variations like \"loved/loving/loves\" → \"love\",\n",
    "   making it easier to identify sentiment patterns across reviews.\n",
    "\n",
    "2. **Handles Contractions**: The regex pattern `[A-Za-z']+` preserves contractions like \"don't\", \"can't\",\n",
    "   which are common in informal review text and carry important sentiment information.\n",
    "\n",
    "3. **Preserves Sentiment Modifiers**: Unlike standard stopword removal, this keeps important words\n",
    "   like \"not\", \"never\", \"very\", \"really\", \"too\" that are crucial for sentiment analysis.\n",
    "   E.g., \"not good\" vs \"good\" have opposite meanings!\n",
    "\n",
    "4. **Reduces Vocabulary Size**: Lemmatization significantly reduces vocabulary while preserving meaning,\n",
    "   which is crucial for machine learning models and topic analysis.\n",
    "\n",
    "5. **Removes Ultra-Short Tokens**: Filtering tokens with length < 2 removes artifacts like standalone\n",
    "   apostrophes or single characters that survived regex, improving data quality.\n",
    "\n",
    "6. **Focuses on Content Words**: By removing most stopwords (while keeping sentiment-critical ones),\n",
    "   we emphasize product-specific terms and quality descriptors most relevant for review analysis.\n",
    "\n",
    "7. **Noise Reduction**: Filtering out numbers and special characters removes rating artifacts\n",
    "   (e.g., \"5/5\", \"10/10\") that don't add semantic value when already captured in structured fields.\n",
    "\n",
    "### Differences from Previous Methods:\n",
    "- Unlike Q3 (regex ver2), this excludes numbers which are noise in sentiment analysis\n",
    "- Unlike Q4 (regex ver3), this applies lemmatization for better normalization\n",
    "- Unlike Q5, this uses a contraction-friendly regex AND preserves sentiment-critical stopwords\n",
    "- **NEW**: Adds sentiment-aware stopword filtering and minimum length requirement\n",
    "- **NEW**: Single integrated pipeline optimized specifically for sentiment analysis tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124eadf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q6 — Custom Tokenizer for Amazon Reviews Results\n",
      "================================================================================\n",
      "\n",
      "--- Row 0 ---\n",
      "Original review_body:\n",
      "Needed this for my percussion class. Works great.\n",
      "\n",
      "Custom Tokenized (token_custom):\n",
      "['need', 'percussion', 'class', 'work', 'great']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 1 ---\n",
      "Original review_body:\n",
      "Ive gone through my share of headphones, these are better than spending $300 or more on dr. dre beats, these have great quality highs-low-bass ect. very comfortable over the ears and it even has lil swivels that you can rotate to sit straight round your neck with the ear parts laying flat on your torso. Fast shipping as well and would recommend getting these!\n",
      "\n",
      "Custom Tokenized (token_custom):\n",
      "['go', 'share', 'headphone', 'well', 'spend', 'dr', 'dre', 'beat', 'great', 'quality', 'high', 'low', 'bass', 'ect', 'very', 'comfortable', 'ear', 'even', 'lil', 'swivel', 'rotate', 'sit', 'straight', 'round', 'neck', 'ear', 'part', 'lay', 'flat', 'torso', 'fast', 'shipping', 'well', 'would', 'recommend', 'get']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 2 ---\n",
      "Original review_body:\n",
      "I recently purchased a very nice Fender Acoustic Guitar (her name is &#34;Suzanne&#34;  -- I name all my guitars).  I purchased this case because it looked nice and the price was very, very nice.  I have owned more than a couple of guitar cases and this is easily the nicest and most beautiful case I have ever owned.  I could not be more satisfied -- no, delighted with this case and this purchase.  You will feel the same about yours once you buy one.  The quality of construction and the solid way it is put together causes me to believe this case will hold up for many years of use.\n",
      "\n",
      "Custom Tokenized (token_custom):\n",
      "['recently', 'purchase', 'very', 'nice', 'fender', 'acoustic', 'guitar', 'name', 'suzanne', 'name', 'guitar', 'purchase', 'case', 'look', 'nice', 'price', 'very', 'very', 'nice', 'couple', 'guitar', 'case', 'easily', 'nice', 'beautiful', 'case', 'ever', 'could', 'not', 'satisfied', 'no', 'delighted', 'case', 'purchase', 'feel', 'buy', 'one', 'quality', 'construction', 'solid', 'way', 'put', 'together', 'cause', 'believe', 'case', 'hold', 'many', 'year', 'use']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 3 ---\n",
      "Original review_body:\n",
      "These drum sticks were almost what they described but with one IIMPORTANT difference... THEY WERE NOT NYLON TIPPED. As a drummer that makes a HUGE difference when playing.<br />  You may well ask why I never returned them. The answer is that I actually would almost lose money to do so. I would only get back the price of the sticks - $16.00. I lose the shipping costs (approximately $11.00) PLUS the return shipping - about the same - and cost of fuel, materials, time and aggravation just to go to the post office to actually accomplish the return shipping. Add in the time it would take to get my money returned to my card and you can see why I decided to sell the sticks to a friend for half their value. I made back more money that way.<br />  I realize it's not Amazon's fault per se, but you might want to let your seller know they are NOT checking their products for comparability with their description of them. I also received notification, via USBS tracking, that it had been delivered when it wasn't and the package got here about a week later. All in all not worth the time and money!<br />Bruce Way\n",
      "\n",
      "Custom Tokenized (token_custom):\n",
      "['drum', 'stick', 'almost', 'describe', 'one', 'iimportant', 'difference', 'not', 'nylon', 'tip', 'drummer', 'make', 'huge', 'difference', 'play', 'br', 'may', 'well', 'ask', 'never', 'return', 'answer', 'actually', 'would', 'almost', 'lose', 'money', 'would', 'get', 'back', 'price', 'stick', 'lose', 'shipping', 'cost', 'approximately', 'plus', 'return', 'ship', 'cost', 'fuel', 'material', 'time', 'aggravation', 'go', 'post', 'office', 'actually', 'accomplish', 'return', 'shipping', 'add', 'time', 'would', 'take', 'get', 'money', 'return', 'card', 'see', 'decide', 'sell', 'stick', 'friend', 'half', 'value', 'make', 'back', 'money', 'way', 'br', 'realize', 'not', 'amazon', 'fault', 'per', 'se', 'might', 'want', 'let', 'seller', 'know', 'not', 'check', 'product', 'comparability', 'description', 'also', 'receive', 'notification', 'via', 'usbs', 'tracking', 'deliver', 'package', 'get', 'week', 'later', 'not', 'worth', 'time', 'money', 'br', 'bruce', 'way']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 4 ---\n",
      "Original review_body:\n",
      "Great microphone. Works well with my Nikon set up.\n",
      "\n",
      "Custom Tokenized (token_custom):\n",
      "['great', 'microphone', 'work', 'well', 'nikon', 'set']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define sentiment-critical words to preserve (important for review sentiment analysis)\n",
    "SENTIMENT_WORDS = {\n",
    "    'not', 'no', 'never', 'neither', 'nor', 'nobody', 'nothing', 'nowhere',\n",
    "    'very', 'really', 'extremely', 'absolutely', 'totally', 'completely',\n",
    "    'too', 'quite', 'rather', 'highly', 'barely', 'hardly', 'scarcely'\n",
    "}\n",
    "\n",
    "# Define custom tokenization function for Amazon reviews\n",
    "def tokenize_amazon_review(text):\n",
    "    \"\"\"\n",
    "    Custom tokenization pipeline optimized for Amazon review sentiment analysis.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Extract tokens using regex pattern that preserves contractions\n",
    "    3. Apply lemmatization via spaCy\n",
    "    4. Filter: keep only alphabetic tokens\n",
    "    5. Remove stopwords BUT preserve sentiment-critical words\n",
    "    6. Remove tokens with length < 2 characters\n",
    "    \n",
    "    Returns: List of cleaned, lemmatized tokens\n",
    "    \"\"\"\n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Extract tokens using regex (letters and apostrophes)\n",
    "    pattern_custom = re.compile(r\"[a-z']+\")\n",
    "    tokens = pattern_custom.findall(text)\n",
    "    \n",
    "    # Join tokens for spaCy processing\n",
    "    text_for_spacy = ' '.join(tokens)\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    doc = nlp(text_for_spacy)\n",
    "    \n",
    "    # Extract lemmas with sentiment-aware filtering\n",
    "    cleaned_tokens = [\n",
    "        token.lemma_.lower() \n",
    "        for token in doc \n",
    "        if token.is_alpha  # Keep only alphabetic tokens\n",
    "        and len(token.lemma_) >= 2  # Minimum length requirement\n",
    "        and (token.lemma_.lower() not in stop_words or token.lemma_.lower() in SENTIMENT_WORDS)  # Remove stopwords EXCEPT sentiment-critical ones\n",
    "    ]\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "# Apply custom tokenizer to review_body\n",
    "df['token_custom'] = df['review_body'].apply(tokenize_amazon_review)\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q6 — Custom Tokenizer for Amazon Reviews Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nCustom Tokenized (token_custom):\\n{df['token_custom'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
