{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951e8048",
   "metadata": {},
   "source": [
    "BUAI 435 Assignment 3 - Tokenization & Lemmatization  \n",
    "Name: Ruihuang Yang  \n",
    "NetID: rxy216  \n",
    "Date: 10/01/2025  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde14869",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a68032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8626aab",
   "metadata": {},
   "source": [
    "### Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926c6b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (904765, 15)\n",
      "\n",
      "Column names:\n",
      "['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date']\n",
      "\n",
      "First few rows of review_body:\n",
      "0          Works very good, but induces ALOT of noise.\n",
      "1               Nice headphones at a reasonable price.\n",
      "2                         removes dust. does not clean\n",
      "3    I purchase these for a friend in return for pl...\n",
      "4                              This is an awesome mic!\n",
      "Name: review_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the Amazon Musical dataset\n",
    "df = pd.read_csv('data/Amazon_Musical.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows of review_body:\")\n",
    "print(df['review_body'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ac35e",
   "metadata": {},
   "source": [
    "## Q1 — Keras Tokenizer (1 point)\n",
    "\n",
    "**Task:**\n",
    "- Import `text_to_word_sequence` from `tensorflow.keras.preprocessing.text`\n",
    "- Convert `review_body` column to string type using `.astype(str)`\n",
    "- Apply `text_to_word_sequence` to each row of `review_body`\n",
    "- Store result in new column `token_keras`\n",
    "- Preview first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cafc3c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q1 — Keras Tokenizer Results\n",
      "================================================================================\n",
      "\n",
      "--- Row 0 ---\n",
      "Original review_body:\n",
      "Works very good, but induces ALOT of noise.\n",
      "\n",
      "Tokenized (token_keras):\n",
      "['works', 'very', 'good', 'but', 'induces', 'alot', 'of', 'noise']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 1 ---\n",
      "Original review_body:\n",
      "Nice headphones at a reasonable price.\n",
      "\n",
      "Tokenized (token_keras):\n",
      "['nice', 'headphones', 'at', 'a', 'reasonable', 'price']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 2 ---\n",
      "Original review_body:\n",
      "removes dust. does not clean\n",
      "\n",
      "Tokenized (token_keras):\n",
      "['removes', 'dust', 'does', 'not', 'clean']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 3 ---\n",
      "Original review_body:\n",
      "I purchase these for a friend in return for playing them for my father and other folks at a local retirement home. Thank you R.BNGR\n",
      "\n",
      "Tokenized (token_keras):\n",
      "['i', 'purchase', 'these', 'for', 'a', 'friend', 'in', 'return', 'for', 'playing', 'them', 'for', 'my', 'father', 'and', 'other', 'folks', 'at', 'a', 'local', 'retirement', 'home', 'thank', 'you', 'r', 'bngr']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 4 ---\n",
      "Original review_body:\n",
      "This is an awesome mic!\n",
      "\n",
      "Tokenized (token_keras):\n",
      "['this', 'is', 'an', 'awesome', 'mic']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert review_body to string type to avoid errors with non-string values\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Apply text_to_word_sequence to each row of review_body\n",
    "df['token_keras'] = df['review_body'].apply(text_to_word_sequence)\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q1 — Keras Tokenizer Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_keras):\\n{df['token_keras'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b174d",
   "metadata": {},
   "source": [
    "## Q2 — Regex Tokenizer Version 1 (1 point)\n",
    "\n",
    "**Task:**\n",
    "- Import the built-in `re` module for regular expressions\n",
    "- Define a compiled regex pattern: `r\"[A-Za-z]+\"`\n",
    "  - Matches one or more English letters (A–Z or a–z)\n",
    "  - Numbers, punctuation, emojis, and symbols are excluded\n",
    "  - Hyphenated words (e.g., cost-effective) will be split (cost, effective)\n",
    "  - No lowercasing or other normalization performed here\n",
    "- Ensure `review_body` is treated as a string: `.astype(str)`\n",
    "- Apply `pattern.findall(x)` to each row of `review_body` with `.apply(...)`\n",
    "- Save the result to a new column named `token_regex_ver1`\n",
    "- Preview by printing `review_body` and `token_regex_ver1` for the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11775dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q2 — Regex Tokenizer Version 1 Results\n",
      "================================================================================\n",
      "\n",
      "--- Row 0 ---\n",
      "Original review_body:\n",
      "Works very good, but induces ALOT of noise.\n",
      "\n",
      "Tokenized (token_regex_ver1):\n",
      "['Works', 'very', 'good', 'but', 'induces', 'ALOT', 'of', 'noise']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 1 ---\n",
      "Original review_body:\n",
      "Nice headphones at a reasonable price.\n",
      "\n",
      "Tokenized (token_regex_ver1):\n",
      "['Nice', 'headphones', 'at', 'a', 'reasonable', 'price']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 2 ---\n",
      "Original review_body:\n",
      "removes dust. does not clean\n",
      "\n",
      "Tokenized (token_regex_ver1):\n",
      "['removes', 'dust', 'does', 'not', 'clean']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 3 ---\n",
      "Original review_body:\n",
      "I purchase these for a friend in return for playing them for my father and other folks at a local retirement home. Thank you R.BNGR\n",
      "\n",
      "Tokenized (token_regex_ver1):\n",
      "['I', 'purchase', 'these', 'for', 'a', 'friend', 'in', 'return', 'for', 'playing', 'them', 'for', 'my', 'father', 'and', 'other', 'folks', 'at', 'a', 'local', 'retirement', 'home', 'Thank', 'you', 'R', 'BNGR']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 4 ---\n",
      "Original review_body:\n",
      "This is an awesome mic!\n",
      "\n",
      "Tokenized (token_regex_ver1):\n",
      "['This', 'is', 'an', 'awesome', 'mic']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define compiled regex pattern to match one or more English letters\n",
    "pattern = re.compile(r\"[A-Za-z]+\")\n",
    "\n",
    "# Ensure review_body is treated as string\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Apply pattern.findall(x) to each row of review_body\n",
    "df['token_regex_ver1'] = df['review_body'].apply(lambda x: pattern.findall(x))\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q2 — Regex Tokenizer Version 1 Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_regex_ver1):\\n{df['token_regex_ver1'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15032c8b",
   "metadata": {},
   "source": [
    "## Q3 — Regex Tokenizer Version 2 (1 point)\n",
    "\n",
    "**Task:**\n",
    "- Define a compiled regex pattern: `r\"\\w+\"`, name the object `pattern2`\n",
    "  - Matches one or more \"word characters\" (letters A–Z/a–z, digits 0–9, underscore `_`)\n",
    "  - Punctuation, emojis, and symbols are excluded\n",
    "  - Hyphenated words will be split\n",
    "  - No lowercasing or normalization performed here\n",
    "- Ensure `review_body` is treated as a string: `.astype(str)`\n",
    "- Apply `pattern2.findall(x)` to each row of `review_body` with `.apply(...)`\n",
    "- Save the result to a new column named `token_regex_ver2`\n",
    "- Preview by printing `review_body` and `token_regex_ver2` for the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1918cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q3 — Regex Tokenizer Version 2 Results\n",
      "================================================================================\n",
      "\n",
      "--- Row 0 ---\n",
      "Original review_body:\n",
      "Works very good, but induces ALOT of noise.\n",
      "\n",
      "Tokenized (token_regex_ver2):\n",
      "['Works', 'very', 'good', 'but', 'induces', 'ALOT', 'of', 'noise']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 1 ---\n",
      "Original review_body:\n",
      "Nice headphones at a reasonable price.\n",
      "\n",
      "Tokenized (token_regex_ver2):\n",
      "['Nice', 'headphones', 'at', 'a', 'reasonable', 'price']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 2 ---\n",
      "Original review_body:\n",
      "removes dust. does not clean\n",
      "\n",
      "Tokenized (token_regex_ver2):\n",
      "['removes', 'dust', 'does', 'not', 'clean']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 3 ---\n",
      "Original review_body:\n",
      "I purchase these for a friend in return for playing them for my father and other folks at a local retirement home. Thank you R.BNGR\n",
      "\n",
      "Tokenized (token_regex_ver2):\n",
      "['I', 'purchase', 'these', 'for', 'a', 'friend', 'in', 'return', 'for', 'playing', 'them', 'for', 'my', 'father', 'and', 'other', 'folks', 'at', 'a', 'local', 'retirement', 'home', 'Thank', 'you', 'R', 'BNGR']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 4 ---\n",
      "Original review_body:\n",
      "This is an awesome mic!\n",
      "\n",
      "Tokenized (token_regex_ver2):\n",
      "['This', 'is', 'an', 'awesome', 'mic']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define compiled regex pattern to match one or more word characters\n",
    "pattern2 = re.compile(r\"\\w+\")\n",
    "\n",
    "# Ensure review_body is treated as string\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Apply pattern2.findall(x) to each row of review_body\n",
    "df['token_regex_ver2'] = df['review_body'].apply(lambda x: pattern2.findall(x))\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q3 — Regex Tokenizer Version 2 Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_regex_ver2):\\n{df['token_regex_ver2'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d9e96d",
   "metadata": {},
   "source": [
    "## Q4 — Regex Tokenizer Version 3 & Removing Some Stop Words (2 points)\n",
    "\n",
    "**Task:**\n",
    "- Define a stoplist as a Python set named **STOPLIST**:\n",
    "  `{\"the\", \"a\", \"an\", \"and\", \"or\", \"to\", \"of\", \"in\", \"for\", \"on\", \"br\"}`\n",
    "- Define a regex pattern object named `pattern3`:\n",
    "  `pattern3 = re.compile(r\"[A-Za-z_']+\")`\n",
    "  - Matches sequences of letters, underscores, or apostrophes\n",
    "  - Allows simple handling of contractions like *don't*\n",
    "  - Numbers and punctuation (other than `'` and `_`) excluded\n",
    "- Ensure `review_body` is treated as string: `.astype(str)`\n",
    "- Convert `review_body` to lowercase: `.str.lower()`\n",
    "- Extract tokens with `pattern3.findall(x)`\n",
    "- Remove any tokens found in the stoplist (`if w not in STOPLIST`)\n",
    "- Save results to a new column: `token_regex_ver3`\n",
    "- Preview by printing both `review_body` and `token_regex_ver3` for the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce85447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q4 — Regex Tokenizer Version 3 & Removing Stop Words Results\n",
      "================================================================================\n",
      "\n",
      "--- Row 0 ---\n",
      "Original review_body:\n",
      "Works very good, but induces ALOT of noise.\n",
      "\n",
      "Tokenized (token_regex_ver3):\n",
      "['works', 'very', 'good', 'but', 'induces', 'alot', 'noise']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 1 ---\n",
      "Original review_body:\n",
      "Nice headphones at a reasonable price.\n",
      "\n",
      "Tokenized (token_regex_ver3):\n",
      "['nice', 'headphones', 'at', 'reasonable', 'price']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 2 ---\n",
      "Original review_body:\n",
      "removes dust. does not clean\n",
      "\n",
      "Tokenized (token_regex_ver3):\n",
      "['removes', 'dust', 'does', 'not', 'clean']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 3 ---\n",
      "Original review_body:\n",
      "I purchase these for a friend in return for playing them for my father and other folks at a local retirement home. Thank you R.BNGR\n",
      "\n",
      "Tokenized (token_regex_ver3):\n",
      "['i', 'purchase', 'these', 'friend', 'return', 'playing', 'them', 'my', 'father', 'other', 'folks', 'at', 'local', 'retirement', 'home', 'thank', 'you', 'r', 'bngr']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Row 4 ---\n",
      "Original review_body:\n",
      "This is an awesome mic!\n",
      "\n",
      "Tokenized (token_regex_ver3):\n",
      "['this', 'is', 'awesome', 'mic']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define stoplist as a Python set\n",
    "STOPLIST = {\"the\", \"a\", \"an\", \"and\", \"or\", \"to\", \"of\", \"in\", \"for\", \"on\", \"br\"}\n",
    "\n",
    "# Define compiled regex pattern to match letters, underscores, or apostrophes\n",
    "pattern3 = re.compile(r\"[A-Za-z_']+\")\n",
    "\n",
    "# Ensure review_body is treated as string and convert to lowercase\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Extract tokens and remove stopwords\n",
    "df['token_regex_ver3'] = df['review_body'].str.lower().apply(\n",
    "    lambda x: [w for w in pattern3.findall(x) if w not in STOPLIST]\n",
    ")\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q4 — Regex Tokenizer Version 3 & Removing Stop Words Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_regex_ver3):\\n{df['token_regex_ver3'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97bcbd7",
   "metadata": {},
   "source": [
    "## Q5 — Lemmatizer for Regex Version 2 (2 points)\n",
    "\n",
    "**Task:**\n",
    "- Import required libraries: `spaCy` and `nltk`\n",
    "- Download the NLTK stopwords list: `nltk.download('stopwords')`\n",
    "- Load English stopwords from NLTK into a Python set `stop_words`\n",
    "- Load the spaCy English model: `en_core_web_sm` → variable `nlp`\n",
    "- Define function **`lemmatize_tokens`** that:\n",
    "  1. Takes a list of tokens as input\n",
    "  2. Joins them into a string for spaCy to process\n",
    "  3. Creates a spaCy `doc` object\n",
    "  4. Extracts each token's lemma (base form)\n",
    "  5. Converts lemma to lowercase\n",
    "  6. Keeps only alphabetic tokens (`token.is_alpha`)\n",
    "  7. Removes tokens found in `stop_words`\n",
    "- Apply `lemmatize_tokens` to the column `token_regex_ver2` to produce a new column `lemmas`\n",
    "- Preview by printing `review_body` and `lemmas` for the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8717fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yangrh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English stopwords from NLTK into a Python set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define lemmatize_tokens function\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens as input and returns lemmatized tokens\n",
    "    with stopwords removed and only alphabetic tokens kept.\n",
    "    \"\"\"\n",
    "    # Join tokens into a string for spaCy to process\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # Create a spaCy doc object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract lemmas, convert to lowercase, keep only alphabetic tokens, remove stopwords\n",
    "    lemmas = [\n",
    "        token.lemma_.lower() \n",
    "        for token in doc \n",
    "        if token.is_alpha and token.lemma_.lower() not in stop_words\n",
    "    ]\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "# Apply lemmatize_tokens to token_regex_ver2 to produce lemmas column\n",
    "df['lemmas'] = df['token_regex_ver2'].apply(lemmatize_tokens)\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q5 — Lemmatizer for Regex Version 2 Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nLemmatized (lemmas):\\n{df['lemmas'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
