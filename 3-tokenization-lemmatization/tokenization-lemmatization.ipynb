{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63880d86",
   "metadata": {},
   "source": [
    "BUAI 435 Assignment 3 - Tokenization & Lemmatization  \n",
    "Name: Ruihuang Yang  \n",
    "NetID: rxy216  \n",
    "Date: 10/01/2025  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a61eb",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908da550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde68f78",
   "metadata": {},
   "source": [
    "### Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2615a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Amazon Musical dataset\n",
    "df = pd.read_csv('data/Amazon_Musical.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Sample 1% of the dataset for faster processing (especially for lemmatization in Q5)\n",
    "df = df.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
    "print(f\"Sampled dataset shape (1%): {df.shape}\")\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows of review_body:\")\n",
    "print(df['review_body'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050cfd12",
   "metadata": {},
   "source": [
    "## Q1 — Keras Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert review_body to string type to avoid errors with non-string values\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Apply text_to_word_sequence to each row of review_body\n",
    "df['token_keras'] = df['review_body'].apply(text_to_word_sequence)\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q1 — Keras Tokenizer Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_keras):\\n{df['token_keras'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31576c4f",
   "metadata": {},
   "source": [
    "## Q2 — Regex Tokenizer Version 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ae9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compiled regex pattern to match one or more English letters\n",
    "pattern = re.compile(r\"[A-Za-z]+\")\n",
    "\n",
    "# Ensure review_body is treated as string\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Apply pattern.findall(x) to each row of review_body\n",
    "df['token_regex_ver1'] = df['review_body'].apply(lambda x: pattern.findall(x))\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q2 — Regex Tokenizer Version 1 Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_regex_ver1):\\n{df['token_regex_ver1'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2eed39",
   "metadata": {},
   "source": [
    "## Q3 — Regex Tokenizer Version 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753508c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compiled regex pattern to match one or more word characters\n",
    "pattern2 = re.compile(r\"\\w+\")\n",
    "\n",
    "# Ensure review_body is treated as string\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Apply pattern2.findall(x) to each row of review_body\n",
    "df['token_regex_ver2'] = df['review_body'].apply(lambda x: pattern2.findall(x))\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q3 — Regex Tokenizer Version 2 Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_regex_ver2):\\n{df['token_regex_ver2'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28021bd",
   "metadata": {},
   "source": [
    "## Q4 — Regex Tokenizer Version 3 & Removing Some Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stoplist as a Python set\n",
    "STOPLIST = {\"the\", \"a\", \"an\", \"and\", \"or\", \"to\", \"of\", \"in\", \"for\", \"on\", \"br\"}\n",
    "\n",
    "# Define compiled regex pattern to match letters, underscores, or apostrophes\n",
    "pattern3 = re.compile(r\"[A-Za-z_']+\")\n",
    "\n",
    "# Ensure review_body is treated as string and convert to lowercase\n",
    "df['review_body'] = df['review_body'].astype(str)\n",
    "\n",
    "# Extract tokens and remove stopwords\n",
    "df['token_regex_ver3'] = df['review_body'].str.lower().apply(\n",
    "    lambda x: [w for w in pattern3.findall(x) if w not in STOPLIST]\n",
    ")\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q4 — Regex Tokenizer Version 3 & Removing Stop Words Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nTokenized (token_regex_ver3):\\n{df['token_regex_ver3'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb90947",
   "metadata": {},
   "source": [
    "## Q5 — Lemmatizer for Regex Version 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English stopwords from NLTK into a Python set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define lemmatize_tokens function\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Takes a list of tokens as input and returns lemmatized tokens\n",
    "    with stopwords removed and only alphabetic tokens kept.\n",
    "    \"\"\"\n",
    "    # Join tokens into a string for spaCy to process\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # Create a spaCy doc object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract lemmas, convert to lowercase, keep only alphabetic tokens, remove stopwords\n",
    "    lemmas = [\n",
    "        token.lemma_.lower() \n",
    "        for token in doc \n",
    "        if token.is_alpha and token.lemma_.lower() not in stop_words\n",
    "    ]\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "# Apply lemmatize_tokens to token_regex_ver2 to produce lemmas column\n",
    "df['lemmas'] = df['token_regex_ver2'].apply(lemmatize_tokens)\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q5 — Lemmatizer for Regex Version 2 Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nLemmatized (lemmas):\\n{df['lemmas'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a1b25",
   "metadata": {},
   "source": [
    "## Q6 — Suggestion of Your Own Tokenizer\n",
    "\n",
    "**My Approach: Sentiment-Aware Lemmatization Pipeline for Amazon Reviews**\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "For Amazon review analysis, I propose a comprehensive tokenization pipeline that combines:\n",
    "1. **Regex tokenization with contraction handling** (`[A-Za-z']+` pattern)\n",
    "2. **Lowercase normalization** for consistency\n",
    "3. **Lemmatization** to reduce words to base forms\n",
    "4. **Sentiment-aware stopword removal** - preserves critical sentiment modifiers\n",
    "5. **Minimum token length filtering** (≥2 characters) to remove noise\n",
    "6. **Alphabetic-only filtering** to remove remaining noise\n",
    "\n",
    "### Why This Approach is Appropriate for Amazon Reviews:\n",
    "\n",
    "1. **Sentiment Analysis Ready**: Lemmatization normalizes variations like \"loved/loving/loves\" → \"love\",\n",
    "   making it easier to identify sentiment patterns across reviews.\n",
    "\n",
    "2. **Handles Contractions**: The regex pattern `[A-Za-z']+` preserves contractions like \"don't\", \"can't\",\n",
    "   which are common in informal review text and carry important sentiment information.\n",
    "\n",
    "3. **Preserves Sentiment Modifiers**: Unlike standard stopword removal, this keeps important words\n",
    "   like \"not\", \"never\", \"very\", \"really\", \"too\" that are crucial for sentiment analysis.\n",
    "   E.g., \"not good\" vs \"good\" have opposite meanings!\n",
    "\n",
    "4. **Reduces Vocabulary Size**: Lemmatization significantly reduces vocabulary while preserving meaning,\n",
    "   which is crucial for machine learning models and topic analysis.\n",
    "\n",
    "5. **Removes Ultra-Short Tokens**: Filtering tokens with length < 2 removes artifacts like standalone\n",
    "   apostrophes or single characters that survived regex, improving data quality.\n",
    "\n",
    "6. **Focuses on Content Words**: By removing most stopwords (while keeping sentiment-critical ones),\n",
    "   we emphasize product-specific terms and quality descriptors most relevant for review analysis.\n",
    "\n",
    "7. **Noise Reduction**: Filtering out numbers and special characters removes rating artifacts\n",
    "   (e.g., \"5/5\", \"10/10\") that don't add semantic value when already captured in structured fields.\n",
    "\n",
    "### Differences from Previous Methods:\n",
    "- Unlike Q3 (regex ver2), this excludes numbers which are noise in sentiment analysis\n",
    "- Unlike Q4 (regex ver3), this applies lemmatization for better normalization\n",
    "- Unlike Q5, this uses a contraction-friendly regex AND preserves sentiment-critical stopwords\n",
    "- **NEW**: Adds sentiment-aware stopword filtering and minimum length requirement\n",
    "- **NEW**: Single integrated pipeline optimized specifically for sentiment analysis tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124eadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentiment-critical words to preserve (important for review sentiment analysis)\n",
    "SENTIMENT_WORDS = {\n",
    "    'not', 'no', 'never', 'neither', 'nor', 'nobody', 'nothing', 'nowhere',\n",
    "    'very', 'really', 'extremely', 'absolutely', 'totally', 'completely',\n",
    "    'too', 'quite', 'rather', 'highly', 'barely', 'hardly', 'scarcely'\n",
    "}\n",
    "\n",
    "# Define custom tokenization function for Amazon reviews\n",
    "def tokenize_amazon_review(text):\n",
    "    \"\"\"\n",
    "    Custom tokenization pipeline optimized for Amazon review sentiment analysis.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Extract tokens using regex pattern that preserves contractions\n",
    "    3. Apply lemmatization via spaCy\n",
    "    4. Filter: keep only alphabetic tokens\n",
    "    5. Remove stopwords BUT preserve sentiment-critical words\n",
    "    6. Remove tokens with length < 2 characters\n",
    "    \n",
    "    Returns: List of cleaned, lemmatized tokens\n",
    "    \"\"\"\n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Extract tokens using regex (letters and apostrophes)\n",
    "    pattern_custom = re.compile(r\"[a-z']+\")\n",
    "    tokens = pattern_custom.findall(text)\n",
    "    \n",
    "    # Join tokens for spaCy processing\n",
    "    text_for_spacy = ' '.join(tokens)\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    doc = nlp(text_for_spacy)\n",
    "    \n",
    "    # Extract lemmas with sentiment-aware filtering\n",
    "    cleaned_tokens = [\n",
    "        token.lemma_.lower() \n",
    "        for token in doc \n",
    "        if token.is_alpha  # Keep only alphabetic tokens\n",
    "        and len(token.lemma_) >= 2  # Minimum length requirement\n",
    "        and (token.lemma_.lower() not in stop_words or token.lemma_.lower() in SENTIMENT_WORDS)  # Remove stopwords EXCEPT sentiment-critical ones\n",
    "    ]\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "# Apply custom tokenizer to review_body\n",
    "df['token_custom'] = df['review_body'].apply(tokenize_amazon_review)\n",
    "\n",
    "# Preview the results for the first 5 rows\n",
    "print(\"=\"*80)\n",
    "print(\"Q6 — Custom Tokenizer for Amazon Reviews Results\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(f\"Original review_body:\\n{df['review_body'].iloc[i]}\")\n",
    "    print(f\"\\nCustom Tokenized (token_custom):\\n{df['token_custom'].iloc[i]}\")\n",
    "    print(\"-\"*80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
