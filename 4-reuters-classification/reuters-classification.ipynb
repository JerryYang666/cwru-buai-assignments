{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783c9414",
   "metadata": {
    "id": "cca12d22"
   },
   "source": [
    "\n",
    "# BUAI 446 – Fall 2024  \n",
    "## Homework 1 (PyTorch Edition)\n",
    "Reuters Newswire Classification  \n",
    "**Name:** Ruihuang Yang  \n",
    "**NetID:** rxy216  \n",
    "**Date:** October 31, 2025  \n",
    "\n",
    "In this homework, you'll build and train a **multiclass text classifier** for the Reuters newswire dataset **using PyTorch**.\n",
    "\n",
    "We'll still use the same dataset (Reuters 46 topics) for consistency, but **all modeling, training, and evaluation must be done in PyTorch**.\n",
    "\n",
    "> Tip: In PyTorch you *don't compile a model*. Instead, you define a `nn.Module`, choose a loss (`nn.CrossEntropyLoss` for single‑label multiclass), pick an optimizer (e.g., `torch.optim.RMSprop`), and write a training loop that iterates over batches. See the official docs if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f1b60",
   "metadata": {
    "id": "93b7aefb"
   },
   "source": [
    "\n",
    "**Helpful references (optional):**  \n",
    "- Build models with `torch.nn` and `nn.Sequential` (PyTorch docs).  \n",
    "- Datasets & DataLoaders (PyTorch docs).  \n",
    "- Training loop basics (PyTorch tutorial).  \n",
    "\n",
    "*(Links included in the assignment PDF on Canvas.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c3393",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94e8ba66",
    "lines_to_next_cell": 2,
    "outputId": "0fc2f218-caa9-49be-cd1b-8f2982eac7b5"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from tensorflow.keras.datasets import reuters  # used **only** to fetch the data\n",
    "from typing import Tuple\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99caef3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84591ad4",
    "lines_to_next_cell": 2,
    "outputId": "168e38b8-d9a1-49be-f94a-e35e65c4dfe5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load Reuters dataset (10,000 most frequent words)\n",
    "# This returns sequences of word indices with variable length, and integer labels in [0, 45].\n",
    "(num_words, num_classes) = (10000, 46)\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=num_words)\n",
    "\n",
    "len(train_data), len(test_data), len(train_labels), len(test_labels), max(train_labels), min(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808762fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "5003c471",
    "outputId": "b4820b59-cd0f-484d-d9be-0f34c4edb610"
   },
   "outputs": [],
   "source": [
    "\n",
    "# See a decoded example to understand the data (optional exploratory cell)\n",
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = {v: k for k, v in word_index.items()}\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[9]])\n",
    "decoded_newswire[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8d6e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c2bc83d",
    "lines_to_next_cell": 2,
    "outputId": "a0b366d8-fe54-40f1-bb5c-4baff8889e9b"
   },
   "outputs": [],
   "source": [
    "\n",
    "def vectorize_sequences(sequences, dimension: int) -> np.ndarray:\n",
    "    \"\"\"Turns a list of sequences into a 2D numpy array of shape (len(sequences), dimension)\n",
    "    where each row is a one‑hot multi-hot of word indices present in the sequence.\"\"\"\n",
    "    result = np.zeros((len(sequences), dimension), dtype=np.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        result[i, np.clip(seq, 0, dimension-1)] = 1.0\n",
    "    return result\n",
    "\n",
    "# 1) Vectorize inputs with one‑hot encoding (multi‑hot presence)\n",
    "x_train = vectorize_sequences(train_data, num_words)\n",
    "x_test = vectorize_sequences(test_data, num_words)\n",
    "\n",
    "# 2) Labels: keep **integer class indices** for PyTorch CrossEntropyLoss\n",
    "y_train = np.array(train_labels, dtype=np.int64)\n",
    "y_test = np.array(test_labels, dtype=np.int64)\n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862c243",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6931acea",
    "outputId": "d0a36ef3-6333-428b-bf68-581470f1387e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create tensors\n",
    "X_train = torch.from_numpy(x_train)\n",
    "y_train_t = torch.from_numpy(y_train)\n",
    "X_test = torch.from_numpy(x_test)\n",
    "y_test_t = torch.from_numpy(y_test)\n",
    "\n",
    "# 3) Hold out 1,000 samples from training for validation\n",
    "val_size = 1000\n",
    "train_size = len(X_train) - val_size\n",
    "train_ds, val_ds = random_split(TensorDataset(X_train, y_train_t), [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test_t), batch_size=batch_size)\n",
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0abb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcf5d232",
    "lines_to_next_cell": 2,
    "outputId": "47ab7050-8493-47a7-dc96-97d8faf6328d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 4) Define an MLP with two hidden layers (64 units each), ReLU, and a 46‑way output (logits)\n",
    "class ReutersMLP(nn.Module):\n",
    "    def __init__(self, in_dim=10000, hidden=64, num_classes=46):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, num_classes)  # logits (no Softmax here)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ReutersMLP(in_dim=num_words, hidden=64, num_classes=num_classes).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5265e65",
   "metadata": {
    "id": "adf7a724"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 5) Choose loss & optimizer\n",
    "# For single‑label multiclass, use CrossEntropyLoss (expects raw logits + **integer** targets).\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)  # RMSprop to mirror the Keras spec\n",
    "\n",
    "def accuracy(logits, targets):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d13592",
   "metadata": {
    "id": "616e85a3",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6-7) Train for 30 epochs, track train/val accuracy\n",
    "epochs = 30\n",
    "history = {\"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    running_acc = 0.0\n",
    "    n_batches = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_acc += accuracy(logits.detach(), yb)\n",
    "        n_batches += 1\n",
    "    train_acc = running_acc / max(1, n_batches)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_acc = 0.0\n",
    "    n_val = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            val_acc += accuracy(logits, yb)\n",
    "            n_val += 1\n",
    "    val_acc = val_acc / max(1, n_val)\n",
    "\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
    "        print(f\"Epoch {epoch:02d} | train_acc={train_acc:.3f} | val_acc={val_acc:.3f}\")\n",
    "\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb313cf0",
   "metadata": {
    "id": "920b6582",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# 8) Evaluate on the test set and predict the class for the first test sample\n",
    "model.eval()\n",
    "test_acc = 0.0\n",
    "n = 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        test_acc += accuracy(logits, yb)\n",
    "        n += 1\n",
    "test_acc = test_acc / max(1, n)\n",
    "print(\"Test accuracy:\", round(test_acc, 3))\n",
    "\n",
    "# Predict the class of the first test sample\n",
    "first_logits = model(torch.from_numpy(x_test[:1]).to(device))\n",
    "pred_class = int(first_logits.argmax(dim=1).item())\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed90a4e",
   "metadata": {
    "id": "f0243615"
   },
   "source": [
    "\n",
    "### What to submit\n",
    "\n",
    "Run all cells so outputs are visible, and submit your notebook as `LastName_FirstName_HW1.ipynb` on Canvas.\n",
    "\n",
    "**Answer these in your notebook (use Markdown cells where appropriate):**\n",
    "1. How many samples are in the train and test sets?  \n",
    "2. How did you vectorize the inputs? Explain why this is appropriate for this problem.  \n",
    "3. Why do we keep labels as integer class indices for `CrossEntropyLoss`? What would change if you used one‑hot labels?  \n",
    "4. Define your MLP as shown. Try **one improvement** (e.g., different hidden size, dropout, weight decay) and report the effect.  \n",
    "5. Plot training vs validation accuracy across 30 epochs. Comment on overfitting/underfitting.  \n",
    "6. Report final **test accuracy** and the predicted class for the first test sample. Briefly interpret the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413fee4f",
   "metadata": {
    "id": "f563e28c"
   },
   "source": [
    "\n",
    "> **Optional challenge (no extra credit):** Try adding `nn.Dropout` and describe the impact.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37aa21",
   "metadata": {
    "id": "asMSlqwz5jIN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
