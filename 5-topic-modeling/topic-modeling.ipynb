{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8f9163",
   "metadata": {
    "id": "25577984"
   },
   "source": [
    "# TF-IDF & Sentiment Analysis & Topic Modeling — 9‑Point Homework\n",
    "  \n",
    "**Dataset:** `Amazon Musical.csv`  \n",
    "Name: Ruihuang Yang  \n",
    "NetID: rxy216  \n",
    "Date: 2025-11-07  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9c81e",
   "metadata": {
    "id": "1649abe8"
   },
   "source": [
    "## 0. Set up & Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbad12b",
   "metadata": {
    "id": "35ad9dfa"
   },
   "outputs": [],
   "source": [
    "# Load basic libraries\n",
    "# Do NOT import these libraries again below\n",
    "# Re-importing (writing inefficient code) will result in deductions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baec17d",
   "metadata": {
    "id": "de31d834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 9048 samples (1% of the original dataset)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Read the CSV file named 'Amazon Musical.csv' into a pandas DataFrame called df\n",
    "df = pd.read_csv('data/Amazon_Musical.csv')\n",
    "\n",
    "# Sample 1% of the dataset for computational efficiency\n",
    "df = df.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
    "print(f\"Working with {len(df)} samples (1% of the original dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7645afd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oelDbiLfGgvb",
    "outputId": "a714a823-9f6f-4171-c9be-cda09bb05f0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9048, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to use the entire dataset for your analysis\n",
    "# Please use the HPC for running this code\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2edb6e",
   "metadata": {
    "id": "tYNxGKIi6tpv"
   },
   "outputs": [],
   "source": [
    "# Load the English NLP model from spaCy\n",
    "# This model provides tokenization, POS tagging, and named entity recognition\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f8850b",
   "metadata": {
    "id": "IqPxz_P56uzX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spaCy NLP processing: 100%|██████████| 9048/9048 [00:40<00:00, 221.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to process text data using spaCy with parallel processing\n",
    "# It extracts token, POS, tag, and lemma information for each review\n",
    "# Do NOT modify this function or its parameters, use it exactly as provided\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def spacy_analyze_pipe(texts):\n",
    "    results = []\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=128, n_process=4), \n",
    "                    total=len(texts), \n",
    "                    desc=\"spaCy NLP processing\"):\n",
    "        tokens = [(token.text, token.pos_, token.tag_, token.lemma_) for token in doc]\n",
    "        results.append(tokens)\n",
    "    return results\n",
    "\n",
    "df[\"spacy_tokens\"] = spacy_analyze_pipe(df[\"review_body\"].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6043b46",
   "metadata": {
    "id": "e2H2kzst6xH_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body  \\\n",
      "0  Needed this for my percussion class. Works great.   \n",
      "1  Ive gone through my share of headphones, these...   \n",
      "\n",
      "                                        spacy_tokens  \n",
      "0  [(Needed, VERB, VBD, need), (this, PRON, DT, t...  \n",
      "1  [(I, PRON, PRP, I), (ve, AUX, VBP, ve), (gone,...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first two rows to check the original text and its spaCy token results\n",
    "print(df[[\"review_body\", \"spacy_tokens\"]].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af941c",
   "metadata": {
    "id": "Z09c_jcf622b"
   },
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer for converting text data into numerical feature vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499d175",
   "metadata": {
    "id": "_E2O5DrG63NH"
   },
   "outputs": [],
   "source": [
    "# Use spaCy's built-in stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Extract and clean lemma tokens from the spaCy results\n",
    "# Keep only alphabetic lemmas, remove stop words, and convert them to lowercase\n",
    "df[\"lemmas\"] = df[\"spacy_tokens\"].apply(\n",
    "    lambda rows: [\n",
    "        lemma.lower().strip()\n",
    "        for (_, _, _, lemma) in rows\n",
    "        if lemma and lemma.strip() and lemma.isalpha() and lemma.lower() not in STOP_WORDS\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9feb7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum TF-IDF scores across all documents\n",
    "# Combine terms and their total TF-IDF scores into a DataFrame\n",
    "# Sort in descending order and return the top N terms\n",
    "# Do NOT modify this function — use it exactly as provided below    \n",
    "def get_top_terms(X, vectorizer, top_n=10):\n",
    "\n",
    "    # Sum TF-IDF scores across all documents\n",
    "    sums = np.asarray(X.sum(axis=0)).ravel()\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Combine into DataFrame and sort descending\n",
    "    df_terms = pd.DataFrame({\"term\": terms, \"score\": sums})\n",
    "    df_terms = df_terms.sort_values(\"score\", ascending=False).head(top_n)\n",
    "\n",
    "    return df_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e18f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine lemma lists into plain text strings\n",
    "# Each document’s tokens are joined into a single string (e.g., [\"great\", \"movie\"] → \"great movie\")\n",
    "df[\"lemmas_text\"] = df[\"lemmas\"].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e837b48e",
   "metadata": {
    "id": "47e8c2d5"
   },
   "source": [
    "## Q1 (1 pt) — TF-IDF with up to 3-grams\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Students must create separate code cells for each task.\n",
    "\n",
    "1. Build the TF-IDF vectorizer (0.5 pt): Using the combined text column (df[\"lemmas_text\"]), create a TF-IDF vectorizer named **vec_list_trigram** that extracts unigrams, bigrams, and trigrams.  \n",
    "\n",
    "This step must follow the specifications below exactly:\n",
    "\n",
    "- Use df[\"lemmas_text\"] as the input, not df[\"lemmas\"].\n",
    "- The variable name must be exactly vec_list_trigram.\n",
    "- The TfidfVectorizer parameters must be: analyzer=\"word\", lowercase=False, ngram_range=(1, 3), sublinear_tf=True\n",
    "- The code should print progress messages.\n",
    "- Do not re-import any libraries that have already been imported above.\n",
    "- Any inefficient, renamed, or altered implementation (e.g., different parameters, variable names) will result in a point deduction.\n",
    "\n",
    "\n",
    "2. Display top trigrams (0.5 pt): After building the TF-IDF matrix, print the top 10 keywords with the highest TF-IDF scores. Use the helper function provided (get_top_terms) exactly.\n",
    "\n",
    "This step must follow the specifications below exactly:\n",
    "\n",
    "- Use the function get_top_terms exactly as provided earlier.\n",
    "- Assign the result to a variable named top_trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF vectorizer with up to trigrams...\n",
      "Fitting and transforming the text data...\n",
      "TF-IDF matrix shape: (9048, 374548)\n",
      "Number of documents: 9048\n",
      "Number of features (terms): 374548\n"
     ]
    }
   ],
   "source": [
    "# Q1.1: Build the TF-IDF vectorizer (0.5 pt)\n",
    "print(\"Building TF-IDF vectorizer with up to trigrams...\")\n",
    "\n",
    "vec_list_trigram = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    lowercase=False,\n",
    "    ngram_range=(1, 3),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "print(\"Fitting and transforming the text data...\")\n",
    "X_list_123g = vec_list_trigram.fit_transform(df[\"lemmas_text\"])\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_list_123g.shape}\")\n",
    "print(f\"Number of documents: {X_list_123g.shape[0]}\")\n",
    "print(f\"Number of features (terms): {X_list_123g.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 terms with highest TF-IDF scores:\n",
      "           term       score\n",
      "126038    great  183.930976\n",
      "121772     good  169.784223\n",
      "367440     work  147.945224\n",
      "298772    sound  103.729620\n",
      "348670      use   99.285626\n",
      "184870     love   96.684884\n",
      "245987    price   83.381801\n",
      "209162     nice   82.713224\n",
      "130474   guitar   82.286599\n",
      "250212  product   82.173855\n"
     ]
    }
   ],
   "source": [
    "# Q1.2: Display top trigrams (0.5 pt)\n",
    "print(\"\\nTop 10 terms with highest TF-IDF scores:\")\n",
    "top_trigrams = get_top_terms(X_list_123g, vec_list_trigram, top_n=10)\n",
    "print(top_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb19d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed data...\n",
      "✓ Saved preprocessed DataFrame to 'data/preprocessed_df.pkl'\n",
      "✓ Saved TF-IDF matrix to 'data/X_list_123g.npz'\n",
      "✓ Saved vectorizer to 'data/vec_list_trigram.pkl'\n",
      "\n",
      "✓ All preprocessing results saved successfully!\n",
      "  You can now start from Q2 by loading these files.\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data and TF-IDF results for quick restart\n",
    "import pickle\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "print(\"Saving preprocessed data...\")\n",
    "\n",
    "# Save the dataframe with all processed columns\n",
    "df.to_pickle('data/preprocessed_df.pkl')\n",
    "print(\"✓ Saved preprocessed DataFrame to 'data/preprocessed_df.pkl'\")\n",
    "\n",
    "# Save the TF-IDF matrix (sparse matrix)\n",
    "save_npz('data/X_list_123g.npz', X_list_123g)\n",
    "print(\"✓ Saved TF-IDF matrix to 'data/X_list_123g.npz'\")\n",
    "\n",
    "# Save the vectorizer\n",
    "with open('data/vec_list_trigram.pkl', 'wb') as f:\n",
    "    pickle.dump(vec_list_trigram, f)\n",
    "print(\"✓ Saved vectorizer to 'data/vec_list_trigram.pkl'\")\n",
    "\n",
    "print(\"\\n✓ All preprocessing results saved successfully!\")\n",
    "print(\"  You can now start from Q2 by loading these files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28fafb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "✓ Loaded DataFrame with 9048 samples\n",
      "✓ Loaded TF-IDF matrix with shape (9048, 374548)\n",
      "✓ Loaded vectorizer with 374548 features\n",
      "\n",
      "✓ All data loaded successfully! Ready to continue from Q2.\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data (use this cell to skip preprocessing and start from Q2)\n",
    "# Uncomment the lines below when you want to load instead of preprocessing\n",
    "import pickle\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "# Load the dataframe\n",
    "df = pd.read_pickle('data/preprocessed_df.pkl')\n",
    "print(f\"✓ Loaded DataFrame with {len(df)} samples\")\n",
    "\n",
    "# Load the TF-IDF matrix\n",
    "X_list_123g = load_npz('data/X_list_123g.npz')\n",
    "print(f\"✓ Loaded TF-IDF matrix with shape {X_list_123g.shape}\")\n",
    "\n",
    "# Load the vectorizer\n",
    "with open('data/vec_list_trigram.pkl', 'rb') as f:\n",
    "    vec_list_trigram = pickle.load(f)\n",
    "print(f\"✓ Loaded vectorizer with {len(vec_list_trigram.get_feature_names_out())} features\")\n",
    "\n",
    "print(\"\\n✓ All data loaded successfully! Ready to continue from Q2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7716adbc",
   "metadata": {},
   "source": [
    "Below is a shared NMF skeleton code that we will use throughout the assignment.\n",
    "Please treat this as the base code and, for each question, only modify the parts that are explicitly requested in the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from sklearn.decomposition import MiniBatchNMF\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6cc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aliases\n",
    "X = X_list_123g\n",
    "vocab = vec_list_trigram.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5969dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN! This is an example code\n",
    "# K = 10          \n",
    "# BATCH = 512     \n",
    "# RANDOM_SEED = 1\n",
    "\n",
    "# nmf = MiniBatchNMF(\n",
    "#     n_components=K,\n",
    "#     init=\"nndsvda\",\n",
    "#     random_state=RANDOM_SEED,\n",
    "#     max_iter=300,\n",
    "#     batch_size=BATCH,\n",
    "# )\n",
    "\n",
    "# W = nmf.fit_transform(X)\n",
    "# H = nmf.components_\n",
    "\n",
    "# print(\"W shape:\", W.shape)\n",
    "# print(\"H shape:\", H.shape)\n",
    "\n",
    "# TOP_N = 10\n",
    "# for k in range(K):\n",
    "#     top_idx = H[k].argsort()[-TOP_N:][::-1]\n",
    "#     top_words = [vocab[i] for i in top_idx]\n",
    "#     print(f\"Topic {k}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dcc4e7",
   "metadata": {},
   "source": [
    "## Q2 (1 pt) — Very coarse topics (K = 2 baseline)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "In this question, you will start with a very coarse topic model.\n",
    "\n",
    "1. Set up a MiniBatchNMF model with:\n",
    "\n",
    "- n_components = 2 (K = 2 topics)\n",
    "\n",
    "- init = \"nndsvda\"\n",
    "\n",
    "- random_state = 42\n",
    "\n",
    "- max_iter = 300\n",
    "\n",
    "- batch_size = 512\n",
    "\n",
    "- Fit the model on X (X_list_123g).\n",
    "\n",
    "2. Print the shapes of W and H.\n",
    "\n",
    "3. For each topic, print the top 10 terms using vocab = vec_list_trigram.get_feature_names_out().\n",
    "\n",
    "4. (Markdown cell) For each topic, look at the keywords and create your own topic name\n",
    "\n",
    "Use the code template above and only change the values needed for this question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03bdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Q2: Training MiniBatchNMF with K=2 topics\n",
      "============================================================\n",
      "\n",
      "Fitting MiniBatchNMF with 2 topics...\n",
      "\n",
      "Matrix shapes:\n",
      "W shape: (9048, 2)\n",
      "H shape: (2, 374548)\n",
      "\n",
      "Top 10 terms for each topic:\n",
      "------------------------------------------------------------\n",
      "Topic 0: great, work, work great, great product, product, price, love, sound, good, use\n",
      "Topic 1: good, good price, good product, price, good quality, quality, sound, product, string, good string\n"
     ]
    }
   ],
   "source": [
    "# Q2: Very coarse topics (K = 2 baseline)\n",
    "print(\"=\" * 60)\n",
    "print(\"Q2: Training MiniBatchNMF with K=2 topics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up aliases (as shown in the template)\n",
    "X = X_list_123g\n",
    "vocab = vec_list_trigram.get_feature_names_out()\n",
    "\n",
    "# Model parameters\n",
    "K = 2\n",
    "BATCH = 512\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Initialize and fit MiniBatchNMF\n",
    "nmf = MiniBatchNMF(\n",
    "    n_components=K,\n",
    "    init=\"nndsvda\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_iter=300,\n",
    "    batch_size=BATCH,\n",
    ")\n",
    "\n",
    "print(f\"\\nFitting MiniBatchNMF with {K} topics...\")\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "# Print shapes\n",
    "print(\"\\nMatrix shapes:\")\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"H shape: {H.shape}\")\n",
    "\n",
    "# Print top 10 terms for each topic\n",
    "print(\"\\nTop 10 terms for each topic:\")\n",
    "print(\"-\" * 60)\n",
    "TOP_N = 10\n",
    "for k in range(K):\n",
    "    top_idx = H[k].argsort()[-TOP_N:][::-1]\n",
    "    top_words = [vocab[i] for i in top_idx]\n",
    "    print(f\"Topic {k}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d4b25",
   "metadata": {},
   "source": [
    "### Q2: Topic Names\n",
    "\n",
    "Based on the top keywords, I would name the topics as follows:\n",
    "\n",
    "- **Topic 0**: The product works well.\n",
    "- **Topic 1**: The product has good price and quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb717aa1",
   "metadata": {},
   "source": [
    "## Q3 (1 pt) — Increase topic count (K = 4)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Now we make the topic structure more fine-grained.\n",
    "\n",
    "1. Set up a MiniBatchNMF model with:\n",
    "\n",
    "- Starting from your Q1 code, change the number of topics to K = 4 (n_components = 4).\n",
    "\n",
    "- Keep all other parameters the same.\n",
    "\n",
    "- Fit the model on X (X_list_123g).\n",
    "\n",
    "2. Print the shapes of W and H.\n",
    "\n",
    "3. For each topic, print the top 10 terms using vocab = vec_list_trigram.get_feature_names_out().\n",
    "\n",
    "4. (Markdown cell) For each topic, look at the keywords and create your own topic name\n",
    "\n",
    "Use the code template above and only change the values needed for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b0b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Q3: Training MiniBatchNMF with K=4 topics\n",
      "============================================================\n",
      "\n",
      "Fitting MiniBatchNMF with 4 topics...\n",
      "\n",
      "Matrix shapes:\n",
      "W shape: (9048, 4)\n",
      "H shape: (4, 374548)\n",
      "\n",
      "Top 10 terms for each topic:\n",
      "------------------------------------------------------------\n",
      "Topic 0: great, work great, great product, work, product, price, good, great price, sound, buy\n",
      "Topic 1: good, good product, good price, good quality, price, quality, product, good string, sound, string\n",
      "Topic 2: work, work great, work perfectly, work fine, fine, work good, perfectly, work advertise, advertise, good\n",
      "Topic 3: nice, love, perfect, sound, like, guitar, use, buy, play, excellent\n"
     ]
    }
   ],
   "source": [
    "# Q3: Increase topic count (K = 4)\n",
    "print(\"=\" * 60)\n",
    "print(\"Q3: Training MiniBatchNMF with K=4 topics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up aliases (as shown in the template)\n",
    "X = X_list_123g\n",
    "vocab = vec_list_trigram.get_feature_names_out()\n",
    "\n",
    "# Model parameters - ONLY change K from 2 to 4\n",
    "K = 4\n",
    "BATCH = 512\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Initialize and fit MiniBatchNMF\n",
    "nmf = MiniBatchNMF(\n",
    "    n_components=K,\n",
    "    init=\"nndsvda\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_iter=300,\n",
    "    batch_size=BATCH,\n",
    ")\n",
    "\n",
    "print(f\"\\nFitting MiniBatchNMF with {K} topics...\")\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "# Print shapes\n",
    "print(\"\\nMatrix shapes:\")\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"H shape: {H.shape}\")\n",
    "\n",
    "# Print top 10 terms for each topic\n",
    "print(\"\\nTop 10 terms for each topic:\")\n",
    "print(\"-\" * 60)\n",
    "TOP_N = 10\n",
    "for k in range(K):\n",
    "    top_idx = H[k].argsort()[-TOP_N:][::-1]\n",
    "    top_words = [vocab[i] for i in top_idx]\n",
    "    print(f\"Topic {k}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea57e6",
   "metadata": {},
   "source": [
    "### Q3: Topic Names\n",
    "\n",
    "Based on the top keywords, I would name the topics as follows:\n",
    "\n",
    "- **Topic 0**: Great Value Products - Products that work great and offer good price\n",
    "- **Topic 1**: Quality Strings and Accessories - Good quality products with focus on instrument strings\n",
    "- **Topic 2**: Products Working as Advertised - Items that work perfectly and meet expectations\n",
    "- **Topic 3**: Positive Musical Instrument Experiences - Love and excellent experiences with guitars and instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f2aa2",
   "metadata": {},
   "source": [
    "## Q4 (1 pt) — Same K=4 but different initialization\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "In this question, we keep K = 4 topics but change how the factorization is initialized.\n",
    "\n",
    "1. Set up a MiniBatchNMF model with:\n",
    "\n",
    "- Change the initialization method from \"nndsvda\" to \"random\".\n",
    "\n",
    "- Keep all other parameters the same.\n",
    "\n",
    "- Fit the model on X (X_list_123g).\n",
    "\n",
    "2. Print the shapes of W and H.\n",
    "\n",
    "3. For each topic, print the top 10 terms using vocab = vec_list_trigram.get_feature_names_out().\n",
    "\n",
    "4. (Markdown cell) For each topic, look at the keywords and create your own topic name\n",
    "\n",
    "Use the code template above and only change the values needed for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e91cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Q4: Training MiniBatchNMF with K=4 topics (random initialization)\n",
      "============================================================\n",
      "\n",
      "Fitting MiniBatchNMF with 4 topics (random init)...\n",
      "\n",
      "Matrix shapes:\n",
      "W shape: (9048, 4)\n",
      "H shape: (4, 374548)\n",
      "\n",
      "Top 10 terms for each topic:\n",
      "------------------------------------------------------------\n",
      "Topic 0: play, heavy string, audio enthusiast personally, definitely hold, box surprised loud, pedal bring, probely break thanhks, look untrained, car pack brim, stereo female\n",
      "Topic 1: use, depend arrange, new squier, overdrive jeff, real doumbek, great, supremely comfy complain, amplifier frankly, school issue, site look smooth\n",
      "Topic 2: turn volume, learn mix, sturdy effect, kit arrive, sound tone recording, laugh function, tax yes hard, snare compromise, job advertise price, octave add\n",
      "Topic 3: use, great, notch avoid fall, plug finally set, guitar grandson lot, readily available different, snap properly, carry handle use, sturdy knob satisfied, double input\n"
     ]
    }
   ],
   "source": [
    "# Q4: Same K=4 but different initialization\n",
    "print(\"=\" * 60)\n",
    "print(\"Q4: Training MiniBatchNMF with K=4 topics (random initialization)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up aliases (as shown in the template)\n",
    "X = X_list_123g\n",
    "vocab = vec_list_trigram.get_feature_names_out()\n",
    "\n",
    "# Model parameters - ONLY change init from \"nndsvda\" to \"random\"\n",
    "K = 4\n",
    "BATCH = 512\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Initialize and fit MiniBatchNMF\n",
    "nmf = MiniBatchNMF(\n",
    "    n_components=K,\n",
    "    init=\"random\",  # CHANGED from \"nndsvda\"\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_iter=300,\n",
    "    batch_size=BATCH,\n",
    ")\n",
    "\n",
    "print(f\"\\nFitting MiniBatchNMF with {K} topics (random init)...\")\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "# Print shapes\n",
    "print(\"\\nMatrix shapes:\")\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"H shape: {H.shape}\")\n",
    "\n",
    "# Print top 10 terms for each topic\n",
    "print(\"\\nTop 10 terms for each topic:\")\n",
    "print(\"-\" * 60)\n",
    "TOP_N = 10\n",
    "for k in range(K):\n",
    "    top_idx = H[k].argsort()[-TOP_N:][::-1]\n",
    "    top_words = [vocab[i] for i in top_idx]\n",
    "    print(f\"Topic {k}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6450557",
   "metadata": {},
   "source": [
    "### Q4: Topic Names\n",
    "\n",
    "Based on the top keywords, I would name the topics as follows:\n",
    "\n",
    "- **Topic 0**: Incoherent/Mixed - Random phrases (heavy string, audio enthusiast, box surprised loud, etc.)\n",
    "- **Topic 1**: Incoherent/Mixed - Scattered terms (use, depend arrange, new squier, overdrive, etc.)\n",
    "- **Topic 2**: Incoherent/Mixed - Unrelated phrases (turn volume, learn mix, laugh function, etc.)\n",
    "- **Topic 3**: Somewhat General Use - Contains \"use\" and \"great\" but mixed with odd phrases\n",
    "\n",
    "**Note**: Random initialization produced very incoherent topics with multi-word phrases stuck together, making interpretation difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4dd90",
   "metadata": {},
   "source": [
    "## Q5 (1 pt) — Same K=4, add change convergence tolerance (early stopping)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Now we still use K = 4 topics, but we change convergence tolerance (early stopping).\n",
    "\n",
    "1. Set up a MiniBatchNMF model with:\n",
    "\n",
    "- K = 4, init = \"random\", random_state = 42, max_iter = 300, batch_size = 512\n",
    "\n",
    "- Add the following parameters to MiniBatchNMF: tol = 1e-3 (make convergence a bit looser than default).\n",
    "\n",
    "- Fit the model on X (X_list_123g).\n",
    "\n",
    "2. Print the shapes of W and H.\n",
    "\n",
    "3. For each topic, print the top 10 terms using vocab = vec_list_trigram.get_feature_names_out().\n",
    "\n",
    "4. (Markdown cell) For each topic, look at the keywords and create your own topic name\n",
    "\n",
    "Use the code template above and only change the values needed for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5c27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Q5: Training MiniBatchNMF with K=4 topics (with early stopping)\n",
      "============================================================\n",
      "\n",
      "Fitting MiniBatchNMF with 4 topics (tol=1e-3)...\n",
      "\n",
      "Matrix shapes:\n",
      "W shape: (9048, 4)\n",
      "H shape: (4, 374548)\n",
      "Number of iterations: 1\n",
      "\n",
      "Top 10 terms for each topic:\n",
      "------------------------------------------------------------\n",
      "Topic 0: heavy string, audio enthusiast personally, box surprised loud, definitely hold, pedal bring, probely break thanhks, look untrained, car pack brim, stereo female, ps\n",
      "Topic 1: depend arrange, new squier, overdrive jeff, real doumbek, supremely comfy complain, school issue, amplifier frankly, site look smooth, business travel lot, teacher look item\n",
      "Topic 2: turn volume, learn mix, sturdy effect, kit arrive, sound tone recording, laugh function, tax yes hard, octave add, snare compromise, job advertise price\n",
      "Topic 3: notch avoid fall, plug finally set, guitar grandson lot, readily available different, snap properly, carry handle use, sturdy knob satisfied, double input, improvisationally, great repair\n"
     ]
    }
   ],
   "source": [
    "# Q5: Same K=4, add early stopping (convergence tolerance)\n",
    "print(\"=\" * 60)\n",
    "print(\"Q5: Training MiniBatchNMF with K=4 topics (with early stopping)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up aliases (as shown in the template)\n",
    "X = X_list_123g\n",
    "vocab = vec_list_trigram.get_feature_names_out()\n",
    "\n",
    "# Model parameters - Same as Q4 but ADD tol parameter\n",
    "K = 4\n",
    "BATCH = 512\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Initialize and fit MiniBatchNMF\n",
    "nmf = MiniBatchNMF(\n",
    "    n_components=K,\n",
    "    init=\"random\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_iter=300,\n",
    "    batch_size=BATCH,\n",
    "    tol=1e-3,  # ADDED: early stopping with looser tolerance\n",
    ")\n",
    "\n",
    "print(f\"\\nFitting MiniBatchNMF with {K} topics (tol=1e-3)...\")\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "# Print shapes\n",
    "print(\"\\nMatrix shapes:\")\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"H shape: {H.shape}\")\n",
    "\n",
    "# Print convergence info\n",
    "print(f\"Number of iterations: {nmf.n_iter_}\")\n",
    "\n",
    "# Print top 10 terms for each topic\n",
    "print(\"\\nTop 10 terms for each topic:\")\n",
    "print(\"-\" * 60)\n",
    "TOP_N = 10\n",
    "for k in range(K):\n",
    "    top_idx = H[k].argsort()[-TOP_N:][::-1]\n",
    "    top_words = [vocab[i] for i in top_idx]\n",
    "    print(f\"Topic {k}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ccf64c",
   "metadata": {},
   "source": [
    "### Q5: Topic Names\n",
    "\n",
    "Based on the top keywords, I would name the topics as follows:\n",
    "\n",
    "- **Topic 0**: Incoherent/Mixed - Random phrases (heavy string, audio enthusiast, box surprised loud, etc.)\n",
    "- **Topic 1**: Incoherent/Mixed - Scattered terms (depend arrange, new squier, overdrive, etc.)\n",
    "- **Topic 2**: Incoherent/Mixed - Unrelated phrases (turn volume, learn mix, sound tone recording, etc.)\n",
    "- **Topic 3**: Somewhat Repair/Guitar Related - Mix of phrases including \"guitar grandson\", \"great repair\"\n",
    "\n",
    "**Note**: Model converged too quickly (only 1 iteration) due to loose tolerance (tol=1e-3), resulting in poorly formed topics similar to Q4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f35915",
   "metadata": {},
   "source": [
    "## Q6 (1 pt) — Same K=4, change batch size & iterations\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Same K = 4, different mini-batch size and max_iter\n",
    "\n",
    "1. Set up a MiniBatchNMF model with:\n",
    "\n",
    "- K = 4, init = \"nndsvda\", random_state = 42\n",
    "\n",
    "- Change: batch_size 128, max_iter 350\n",
    "\n",
    "- Fit the model on X (X_list_123g).\n",
    "\n",
    "2. Print the shapes of W and H.\n",
    "\n",
    "3. For each topic, print the top 10 terms using vocab = vec_list_trigram.get_feature_names_out().\n",
    "\n",
    "4. (Markdown cell) For each topic, look at the keywords and create your own topic name\n",
    "\n",
    "Use the code template above and only change the values needed for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a6c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Q6: Training MiniBatchNMF with K=4 topics (smaller batch, more iterations)\n",
      "============================================================\n",
      "\n",
      "Fitting MiniBatchNMF with 4 topics (batch_size=128, max_iter=350)...\n",
      "\n",
      "Matrix shapes:\n",
      "W shape: (9048, 4)\n",
      "H shape: (4, 374548)\n",
      "Number of iterations: 1\n",
      "\n",
      "Top 10 terms for each topic:\n",
      "------------------------------------------------------------\n",
      "Topic 0: great, work, work great, good, product, great product, price, sound, use, love\n",
      "Topic 1: good, good price, good product, good quality, price, quality, sound, good string, nice, string\n",
      "Topic 2: work, work great, good, work perfectly, work fine, work good, work advertise, perfectly, fine, advertise\n",
      "Topic 3: nice, love, perfect, sound, like, guitar, use, buy, play, excellent\n"
     ]
    }
   ],
   "source": [
    "# Q6: Same K=4, change batch size & iterations\n",
    "print(\"=\" * 60)\n",
    "print(\"Q6: Training MiniBatchNMF with K=4 topics (smaller batch, more iterations)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up aliases (as shown in the template)\n",
    "X = X_list_123g\n",
    "vocab = vec_list_trigram.get_feature_names_out()\n",
    "\n",
    "# Model parameters - Change batch_size and max_iter, init back to \"nndsvda\"\n",
    "K = 4\n",
    "BATCH = 128  # CHANGED from 512 to 128\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Initialize and fit MiniBatchNMF\n",
    "nmf = MiniBatchNMF(\n",
    "    n_components=K,\n",
    "    init=\"nndsvda\",  # Back to \"nndsvda\"\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_iter=350,  # CHANGED from 300 to 350\n",
    "    batch_size=BATCH,\n",
    ")\n",
    "\n",
    "print(f\"\\nFitting MiniBatchNMF with {K} topics (batch_size={BATCH}, max_iter=350)...\")\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "# Print shapes\n",
    "print(\"\\nMatrix shapes:\")\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"H shape: {H.shape}\")\n",
    "\n",
    "# Print convergence info\n",
    "print(f\"Number of iterations: {nmf.n_iter_}\")\n",
    "\n",
    "# Print top 10 terms for each topic\n",
    "print(\"\\nTop 10 terms for each topic:\")\n",
    "print(\"-\" * 60)\n",
    "TOP_N = 10\n",
    "for k in range(K):\n",
    "    top_idx = H[k].argsort()[-TOP_N:][::-1]\n",
    "    top_words = [vocab[i] for i in top_idx]\n",
    "    print(f\"Topic {k}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247acb13",
   "metadata": {},
   "source": [
    "### Q6: Topic Names\n",
    "\n",
    "Based on the top keywords, I would name the topics as follows:\n",
    "\n",
    "- **Topic 0**: General Positive Reviews - Products that work great, good quality, and loved by users\n",
    "- **Topic 1**: Quality and Value Focus - Good price, quality products, especially strings and accessories\n",
    "- **Topic 2**: Functional Performance - Products working perfectly, fine, and as advertised\n",
    "- **Topic 3**: Musical Instrument Satisfaction - Love, perfect sound, excellent guitar experiences\n",
    "\n",
    "**Note**: Despite only 1 iteration, nndsvda initialization produced coherent, interpretable topics similar to Q3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690e9c6",
   "metadata": {},
   "source": [
    "## Q7 (3 pt) — Same K=4, change batch size & iterations\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Based on the printed top terms from each question (Q1–Q5), write a short reflection (1–2 paragraphs or bullet points) addressing:\n",
    "\n",
    "1. Effect of the number of topics  (0.5 pt):\n",
    "\n",
    "- How do the K = 2 topics  differ from the K = 4 topics ?\n",
    "\n",
    "- Do the K = 2 topics look too broad or mixed?\n",
    "\n",
    "2. Effect of initialization  (0.5 pt):\n",
    "\n",
    "- Compare the K = 4 topics from \"nndsvda\"  and \"random\" .\n",
    "\n",
    "- Which one looks more stable and coherent?\n",
    "\n",
    "3. Effect of early stopping (0.5 pt):\n",
    "\n",
    "- Compared to the baseline model, do the topics with the new stopping criterion (tolerance) look more or less stable and interpretable?\n",
    "\n",
    "- Do you observe any trade-off between runtime and topic quality (for example, similar topics but faster, or slightly noisier topics but shorter training time)?\n",
    "\n",
    "4. Effect of batch size and iterations (0.5 pt):\n",
    "\n",
    "- When you changed batch_size and max_iter , did the topics change noticeably?\n",
    "\n",
    "- Do you think smaller batches + more iterations made the model more stable, less stable, or similar?\n",
    "\n",
    "5. Summarize which configuration (Q2–Q5) you would choose as your “final” topic model for this dataset and briefly justify your choice (1 pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901255d2",
   "metadata": {},
   "source": [
    "#your answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
