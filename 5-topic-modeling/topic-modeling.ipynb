{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e5af5b",
   "metadata": {
    "id": "25577984"
   },
   "source": [
    "# TF-IDF & Sentiment Analysis & Topic Modeling — 9‑Point Homework\n",
    "  \n",
    "**Dataset:** `Amazon Musical.csv`  \n",
    "Name: Ruihuang Yang  \n",
    "NetID: rxy216  \n",
    "Date: 2025-11-07  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10de28c",
   "metadata": {
    "id": "1649abe8"
   },
   "source": [
    "## 0. Set up & Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d812fb",
   "metadata": {
    "id": "35ad9dfa"
   },
   "outputs": [],
   "source": [
    "# Load basic libraries\n",
    "# Do NOT import these libraries again below\n",
    "# Re-importing (writing inefficient code) will result in deductions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e27101",
   "metadata": {
    "id": "de31d834"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Read the CSV file named 'Amazon Musical.csv' into a pandas DataFrame called df\n",
    "df = pd.read_csv('Amazon Musical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30ddbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oelDbiLfGgvb",
    "outputId": "a714a823-9f6f-4171-c9be-cda09bb05f0a"
   },
   "outputs": [],
   "source": [
    "# Make sure to use the entire dataset for your analysis\n",
    "# Please use the HPC for running this code\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48861f05",
   "metadata": {
    "id": "tYNxGKIi6tpv"
   },
   "outputs": [],
   "source": [
    "# Load the English NLP model from spaCy\n",
    "# This model provides tokenization, POS tagging, and named entity recognition\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa47d7",
   "metadata": {
    "id": "IqPxz_P56uzX"
   },
   "outputs": [],
   "source": [
    "# Define a function to process text data using spaCy with parallel processing\n",
    "# It extracts token, POS, tag, and lemma information for each review\n",
    "# Do NOT modify this function or its parameters, use it exactly as provided\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def spacy_analyze_pipe(texts):\n",
    "    results = []\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=128, n_process=4), \n",
    "                    total=len(texts), \n",
    "                    desc=\"spaCy NLP processing\"):\n",
    "        tokens = [(token.text, token.pos_, token.tag_, token.lemma_) for token in doc]\n",
    "        results.append(tokens)\n",
    "    return results\n",
    "\n",
    "df[\"spacy_tokens\"] = spacy_analyze_pipe(df[\"review_body\"].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed03e4",
   "metadata": {
    "id": "e2H2kzst6xH_"
   },
   "outputs": [],
   "source": [
    "# Display the first two rows to check the original text and its spaCy token results\n",
    "print(df[[\"review_body\", \"spacy_tokens\"]].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c4cb61",
   "metadata": {
    "id": "Z09c_jcf622b"
   },
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer for converting text data into numerical feature vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140535d",
   "metadata": {
    "id": "_E2O5DrG63NH"
   },
   "outputs": [],
   "source": [
    "# Extract and clean lemma tokens from the spaCy results\n",
    "# Keep only alphabetic lemmas and convert them to lowercase\n",
    "df[\"lemmas\"] = df[\"spacy_tokens\"].apply(\n",
    "    lambda rows: [\n",
    "        lemma.lower().strip()\n",
    "        for (_, _, _, lemma) in rows\n",
    "        if lemma and lemma.strip() and lemma.isalpha()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0015df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum TF-IDF scores across all documents\n",
    "# Combine terms and their total TF-IDF scores into a DataFrame\n",
    "# Sort in descending order and return the top N terms\n",
    "# Do NOT modify this function — use it exactly as provided below    \n",
    "def get_top_terms(X, vectorizer, top_n=10):\n",
    "\n",
    "    # Sum TF-IDF scores across all documents\n",
    "    sums = np.asarray(X.sum(axis=0)).ravel()\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Combine into DataFrame and sort descending\n",
    "    df_terms = pd.DataFrame({\"term\": terms, \"score\": sums})\n",
    "    df_terms = df_terms.sort_values(\"score\", ascending=False).head(top_n)\n",
    "\n",
    "    return df_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine lemma lists into plain text strings\n",
    "# Each document’s tokens are joined into a single string (e.g., [\"great\", \"movie\"] → \"great movie\")\n",
    "df[\"lemmas_text\"] = df[\"lemmas\"].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9fc40",
   "metadata": {
    "id": "47e8c2d5"
   },
   "source": [
    "## Q1 (1 pt) — TF-IDF with up to 3-grams\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Students must create separate code cells for each task.\n",
    "\n",
    "1. Build the TF-IDF vectorizer (0.5 pt): Using the combined text column (df[\"lemmas_text\"]), create a TF-IDF vectorizer named **vec_list_trigram** that extracts unigrams, bigrams, and trigrams.  \n",
    "\n",
    "This step must follow the specifications below exactly:\n",
    "\n",
    "- Use df[\"lemmas_text\"] as the input, not df[\"lemmas\"].\n",
    "- The variable name must be exactly vec_list_trigram.\n",
    "- The TfidfVectorizer parameters must be: analyzer=\"word\", lowercase=False, ngram_range=(1, 3), sublinear_tf=True\n",
    "- The code should print progress messages.\n",
    "- Do not re-import any libraries that have already been imported above.\n",
    "- Any inefficient, renamed, or altered implementation (e.g., different parameters, variable names) will result in a point deduction.\n",
    "\n",
    "\n",
    "2. Display top trigrams (0.5 pt): After building the TF-IDF matrix, print the top 10 keywords with the highest TF-IDF scores. Use the helper function provided (get_top_terms) exactly.\n",
    "\n",
    "This step must follow the specifications below exactly:\n",
    "\n",
    "- Use the function get_top_terms exactly as provided earlier.\n",
    "- Assign the result to a variable named top_trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692998fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79662fb4",
   "metadata": {},
   "source": [
    "Below is a shared NMF skeleton code that we will use throughout the assignment.\n",
    "Please treat this as the base code and, for each question, only modify the parts that are explicitly requested in the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b060af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from sklearn.decomposition import MiniBatchNMF\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aliases\n",
    "X = X_list_123g\n",
    "vocab = vec_list_trigram.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad846504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN! This is an example code\n",
    "K = 10          \n",
    "BATCH = 512     \n",
    "RANDOM_SEED = 1\n",
    "\n",
    "nmf = MiniBatchNMF(\n",
    "    n_components=K,\n",
    "    init=\"nndsvda\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_iter=300,\n",
    "    batch_size=BATCH,\n",
    ")\n",
    "\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "print(\"W shape:\", W.shape)\n",
    "print(\"H shape:\", H.shape)\n",
    "\n",
    "TOP_N = 10\n",
    "for k in range(K):\n",
    "    top_idx = H[k].argsort()[-TOP_N:][::-1]\n",
    "    top_words = [vocab[i] for i in top_idx]\n",
    "    print(f\"Topic {k}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc84762",
   "metadata": {},
   "source": [
    "## Q2 (1 pt) — Very coarse topics (K = 2 baseline)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "In this question, you will start with a very coarse topic model.\n",
    "\n",
    "1. Set up a MiniBatchNMF model with:\n",
    "\n",
    "- n_components = 2 (K = 2 topics)\n",
    "\n",
    "- init = \"nndsvda\"\n",
    "\n",
    "- random_state = 42\n",
    "\n",
    "- max_iter = 300\n",
    "\n",
    "- batch_size = 512\n",
    "\n",
    "- Fit the model on X (X_list_123g).\n",
    "\n",
    "2. Print the shapes of W and H.\n",
    "\n",
    "3. For each topic, print the top 10 terms using vocab = vec_list_trigram.get_feature_names_out().\n",
    "\n",
    "4. (Markdown cell) For each topic, look at the keywords and create your own topic name\n",
    "\n",
    "Use the code template above and only change the values needed for this question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca59fe4",
   "metadata": {},
   "source": [
    "## Q3 (1 pt) — Increase topic count (K = 4)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Now we make the topic structure more fine-grained.\n",
    "\n",
    "1. Set up a MiniBatchNMF model with:\n",
    "\n",
    "- Starting from your Q1 code, change the number of topics to K = 4 (n_components = 4).\n",
    "\n",
    "- Keep all other parameters the same.\n",
    "\n",
    "- Fit the model on X (X_list_123g).\n",
    "\n",
    "2. Print the shapes of W and H.\n",
    "\n",
    "3. For each topic, print the top 10 terms using vocab = vec_list_trigram.get_feature_names_out().\n",
    "\n",
    "4. (Markdown cell) For each topic, look at the keywords and create your own topic name\n",
    "\n",
    "Use the code template above and only change the values needed for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f48e8",
   "metadata": {},
   "source": [
    "## Q4 (1 pt) — Same K=4 but different initialization\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "In this question, we keep K = 4 topics but change how the factorization is initialized.\n",
    "\n",
    "1. Set up a MiniBatchNMF model with:\n",
    "\n",
    "- Change the initialization method from \"nndsvda\" to \"random\".\n",
    "\n",
    "- Keep all other parameters the same.\n",
    "\n",
    "- Fit the model on X (X_list_123g).\n",
    "\n",
    "2. Print the shapes of W and H.\n",
    "\n",
    "3. For each topic, print the top 10 terms using vocab = vec_list_trigram.get_feature_names_out().\n",
    "\n",
    "4. (Markdown cell) For each topic, look at the keywords and create your own topic name\n",
    "\n",
    "Use the code template above and only change the values needed for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6dcccd",
   "metadata": {},
   "source": [
    "## Q5 (1 pt) — Same K=4, add change convergence tolerance (early stopping)\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Now we still use K = 4 topics, but we change convergence tolerance (early stopping).\n",
    "\n",
    "1. Set up a MiniBatchNMF model with:\n",
    "\n",
    "- K = 4, init = \"random\", random_state = 42, max_iter = 300, batch_size = 512\n",
    "\n",
    "- Add the following parameters to MiniBatchNMF: tol = 1e-3 (make convergence a bit looser than default).\n",
    "\n",
    "- Fit the model on X (X_list_123g).\n",
    "\n",
    "2. Print the shapes of W and H.\n",
    "\n",
    "3. For each topic, print the top 10 terms using vocab = vec_list_trigram.get_feature_names_out().\n",
    "\n",
    "4. (Markdown cell) For each topic, look at the keywords and create your own topic name\n",
    "\n",
    "Use the code template above and only change the values needed for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd2e68",
   "metadata": {},
   "source": [
    "## Q6 (1 pt) — Same K=4, change batch size & iterations\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Same K = 4, different mini-batch size and max_iter\n",
    "\n",
    "1. Set up a MiniBatchNMF model with:\n",
    "\n",
    "- K = 4, init = \"nndsvda\", random_state = 42\n",
    "\n",
    "- Change: batch_size 128, max_iter 350\n",
    "\n",
    "- Fit the model on X (X_list_123g).\n",
    "\n",
    "2. Print the shapes of W and H.\n",
    "\n",
    "3. For each topic, print the top 10 terms using vocab = vec_list_trigram.get_feature_names_out().\n",
    "\n",
    "4. (Markdown cell) For each topic, look at the keywords and create your own topic name\n",
    "\n",
    "Use the code template above and only change the values needed for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ab41a",
   "metadata": {},
   "source": [
    "## Q7 (3 pt) — Same K=4, change batch size & iterations\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "Based on the printed top terms from each question (Q1–Q5), write a short reflection (1–2 paragraphs or bullet points) addressing:\n",
    "\n",
    "1. Effect of the number of topics  (0.5 pt):\n",
    "\n",
    "- How do the K = 2 topics  differ from the K = 4 topics ?\n",
    "\n",
    "- Do the K = 2 topics look too broad or mixed?\n",
    "\n",
    "2. Effect of initialization  (0.5 pt):\n",
    "\n",
    "- Compare the K = 4 topics from \"nndsvda\"  and \"random\" .\n",
    "\n",
    "- Which one looks more stable and coherent?\n",
    "\n",
    "3. Effect of early stopping (0.5 pt):\n",
    "\n",
    "- Compared to the baseline model, do the topics with the new stopping criterion (tolerance) look more or less stable and interpretable?\n",
    "\n",
    "- Do you observe any trade-off between runtime and topic quality (for example, similar topics but faster, or slightly noisier topics but shorter training time)?\n",
    "\n",
    "4. Effect of batch size and iterations (0.5 pt):\n",
    "\n",
    "- When you changed batch_size and max_iter , did the topics change noticeably?\n",
    "\n",
    "- Do you think smaller batches + more iterations made the model more stable, less stable, or similar?\n",
    "\n",
    "5. Summarize which configuration (Q2–Q5) you would choose as your “final” topic model for this dataset and briefly justify your choice (1 pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f790a",
   "metadata": {},
   "source": [
    "#your answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
