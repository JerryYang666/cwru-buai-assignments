{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec26dd97",
   "metadata": {},
   "source": [
    "BUAI 446 Assignment 1 - Akron Zoo\n",
    "Name: Ruihuang Yang\n",
    "NetID: rxy216\n",
    "Date: 09/07/2025\n",
    "Disclaimer: Some code in this document was generated with assistance from Claude 4.0 Sonnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f6e48",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d43586",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de7b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test datasets\n",
    "train_data = pd.read_csv(\"data/ZOOLOG1-TRAIN-2025.csv\")\n",
    "test_data = pd.read_csv(\"data/ZOOLOG1-TEST-2025.csv\")\n",
    "\n",
    "print(\"Training Data Shape:\", train_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca0056",
   "metadata": {},
   "source": [
    "#### Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016271b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of training data\n",
    "print(\"Training Data - First 5 rows:\")\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8075803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information\n",
    "print(\"\\nTraining Data - Column Info:\")\n",
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22034f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"\\nTraining Data - Descriptive Statistics:\")\n",
    "print(train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c68430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in training data:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "print(\"\\nTarget variable (UPD) distribution in training data:\")\n",
    "print(train_data[\"UPD\"].value_counts())\n",
    "print(f\"\\nUpgrade rate: {train_data['UPD'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a83d8f4",
   "metadata": {},
   "source": [
    "### EDA Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd3f29",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up beautiful styling for plots\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"axes.titlesize\"] = 16\n",
    "plt.rcParams[\"axes.labelsize\"] = 14\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 12\n",
    "plt.rcParams[\"legend.fontsize\"] = 12\n",
    "\n",
    "# Define a professional color palette\n",
    "colors = [\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#592E83\"]\n",
    "upgrade_colors = [\"#E74C3C\", \"#2ECC71\"]  # Red for No Upgrade, Green for Upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd32d7f",
   "metadata": {},
   "source": [
    "#### Dataset Balance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a7324",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create a comprehensive view of dataset balance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart showing upgrade distribution\n",
    "upgrade_counts = train_data[\"UPD\"].value_counts().sort_index()\n",
    "bars = ax1.bar(\n",
    "    [\"No Upgrade\", \"Upgrade\"],\n",
    "    upgrade_counts.values,\n",
    "    color=upgrade_colors,\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, count) in enumerate(zip(bars, upgrade_counts.values)):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 5,\n",
    "        f\"{count}\\n({count / len(train_data) * 100:.1f}%)\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "ax1.set_title(\n",
    "    \"Customer Upgrade Distribution\\n(Training Dataset)\", fontweight=\"bold\", pad=20\n",
    ")\n",
    "ax1.set_ylabel(\"Number of Customers\", fontweight=\"bold\")\n",
    "ax1.set_ylim(0, max(upgrade_counts.values) * 1.15)\n",
    "ax1.grid(axis=\"y\", alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# Add sample size annotation\n",
    "ax1.text(\n",
    "    0.5,\n",
    "    -0.15,\n",
    "    f\"Total Sample Size: {len(train_data)} customers\",\n",
    "    transform=ax1.transAxes,\n",
    "    ha=\"center\",\n",
    "    fontsize=12,\n",
    "    style=\"italic\",\n",
    ")\n",
    "\n",
    "# Pie chart for visual balance\n",
    "wedges, texts, autotexts = ax2.pie(\n",
    "    upgrade_counts.values,\n",
    "    labels=[\"No Upgrade\\n(50.0%)\", \"Upgrade\\n(50.0%)\"],\n",
    "    colors=upgrade_colors,\n",
    "    autopct=\"\",\n",
    "    startangle=90,\n",
    "    textprops={\"fontsize\": 14, \"fontweight\": \"bold\"},\n",
    "    wedgeprops={\"edgecolor\": \"white\", \"linewidth\": 3},\n",
    ")\n",
    "\n",
    "ax2.set_title(\n",
    "    \"Dataset Balance Visualization\\n(Perfectly Balanced)\", fontweight=\"bold\", pad=20\n",
    ")\n",
    "\n",
    "# Add center circle for donut effect\n",
    "centre_circle = plt.Circle((0, 0), 0.50, fc=\"white\", linewidth=2, edgecolor=\"gray\")\n",
    "ax2.add_artist(centre_circle)\n",
    "ax2.text(\n",
    "    0,\n",
    "    0,\n",
    "    \"Balanced\\nDataset\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"gray\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Dataset Balance Summary:\")\n",
    "print(\n",
    "    f\"‚Ä¢ No Upgrade: {upgrade_counts[0]} customers ({upgrade_counts[0] / len(train_data) * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"‚Ä¢ Upgrade: {upgrade_counts[1]} customers ({upgrade_counts[1] / len(train_data) * 100:.1f}%)\"\n",
    ")\n",
    "print(\"‚Ä¢ Perfect balance ratio: 1:1\")\n",
    "print(f\"‚Ä¢ Total sample size: {len(train_data)} customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c03a7c",
   "metadata": {},
   "source": [
    "#### Distance vs Upgrade Behavior Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9d418",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create distance labels for better visualization\n",
    "distance_labels = {1: \"< 10 min\", 2: \"10-20 min\", 3: \"21-30 min\", 4: \"> 30 min\"}\n",
    "train_data[\"dist_label\"] = train_data[\"dist\"].map(distance_labels)\n",
    "\n",
    "# Calculate upgrade rates by distance\n",
    "dist_analysis = (\n",
    "    train_data.groupby(\"dist_label\").agg({\"UPD\": [\"count\", \"sum\", \"mean\"]}).round(3)\n",
    ")\n",
    "dist_analysis.columns = [\"Total_Customers\", \"Upgrades\", \"Upgrade_Rate\"]\n",
    "dist_analysis = dist_analysis.reindex(\n",
    "    [\"< 10 min\", \"10-20 min\", \"21-30 min\", \"> 30 min\"]\n",
    ")\n",
    "\n",
    "print(\"Distance vs Upgrade Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "for dist, row in dist_analysis.iterrows():\n",
    "    print(\n",
    "        f\"{dist:12s}: {row['Total_Customers']:3.0f} customers, \"\n",
    "        f\"{row['Upgrades']:3.0f} upgrades ({row['Upgrade_Rate'] * 100:5.1f}%)\"\n",
    "    )\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac851f8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create comprehensive distance vs upgrade visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "# 1. Stacked Bar Chart showing absolute numbers\n",
    "dist_crosstab = pd.crosstab(train_data[\"dist_label\"], train_data[\"UPD\"])\n",
    "dist_crosstab = dist_crosstab.reindex(\n",
    "    [\"< 10 min\", \"10-20 min\", \"21-30 min\", \"> 30 min\"]\n",
    ")\n",
    "\n",
    "bars1 = ax1.bar(\n",
    "    dist_crosstab.index,\n",
    "    dist_crosstab[0],\n",
    "    color=upgrade_colors[0],\n",
    "    alpha=0.8,\n",
    "    label=\"No Upgrade\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    ")\n",
    "bars2 = ax1.bar(\n",
    "    dist_crosstab.index,\n",
    "    dist_crosstab[1],\n",
    "    bottom=dist_crosstab[0],\n",
    "    color=upgrade_colors[1],\n",
    "    alpha=0.8,\n",
    "    label=\"Upgrade\",\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=1,\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(dist_crosstab.iterrows()):\n",
    "    total = row.sum()\n",
    "    ax1.text(\n",
    "        i,\n",
    "        row[0] / 2,\n",
    "        f\"{row[0]}\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        color=\"white\",\n",
    "    )\n",
    "    ax1.text(\n",
    "        i,\n",
    "        row[0] + row[1] / 2,\n",
    "        f\"{row[1]}\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        color=\"white\",\n",
    "    )\n",
    "    ax1.text(\n",
    "        i,\n",
    "        total + 5,\n",
    "        f\"Total: {total}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "ax1.set_title(\n",
    "    \"Customer Distribution by Distance\\n(Absolute Numbers)\", fontweight=\"bold\", pad=20\n",
    ")\n",
    "ax1.set_ylabel(\"Number of Customers\", fontweight=\"bold\")\n",
    "ax1.legend(loc=\"upper right\", framealpha=0.9)\n",
    "ax1.grid(axis=\"y\", alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# 2. Upgrade Rate by Distance (Percentage)\n",
    "upgrade_rates = dist_analysis[\"Upgrade_Rate\"] * 100\n",
    "colors_gradient = [\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\", \"#96CEB4\"]\n",
    "\n",
    "bars = ax2.bar(\n",
    "    upgrade_rates.index,\n",
    "    upgrade_rates.values,\n",
    "    color=colors_gradient,\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for bar, rate in zip(bars, upgrade_rates.values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 1,\n",
    "        f\"{rate:.1f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "ax2.set_title(\n",
    "    \"Upgrade Rate by Distance from Zoo\\n(Percentage)\", fontweight=\"bold\", pad=20\n",
    ")\n",
    "ax2.set_ylabel(\"Upgrade Rate (%)\", fontweight=\"bold\")\n",
    "ax2.set_ylim(0, max(upgrade_rates.values) * 1.2)\n",
    "ax2.grid(axis=\"y\", alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# Add average line\n",
    "avg_rate = train_data[\"UPD\"].mean() * 100\n",
    "ax2.axhline(y=avg_rate, color=\"red\", linestyle=\"--\", alpha=0.7, linewidth=2)\n",
    "ax2.text(\n",
    "    0.02,\n",
    "    avg_rate + 1,\n",
    "    f\"Overall Average: {avg_rate:.1f}%\",\n",
    "    transform=ax2.get_yaxis_transform(),\n",
    "    fontsize=10,\n",
    "    color=\"red\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# 3. Grouped Bar Chart for detailed comparison\n",
    "x = np.arange(len(dist_analysis.index))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax3.bar(\n",
    "    x - width / 2,\n",
    "    dist_analysis[\"Total_Customers\"],\n",
    "    width,\n",
    "    label=\"Total Customers\",\n",
    "    color=\"#3498DB\",\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"white\",\n",
    ")\n",
    "bars2 = ax3.bar(\n",
    "    x + width / 2,\n",
    "    dist_analysis[\"Upgrades\"],\n",
    "    width,\n",
    "    label=\"Upgrades\",\n",
    "    color=\"#2ECC71\",\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"white\",\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "    ax3.text(\n",
    "        bar1.get_x() + bar1.get_width() / 2.0,\n",
    "        bar1.get_height() + 2,\n",
    "        f\"{int(bar1.get_height())}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "    ax3.text(\n",
    "        bar2.get_x() + bar2.get_width() / 2.0,\n",
    "        bar2.get_height() + 2,\n",
    "        f\"{int(bar2.get_height())}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "ax3.set_title(\n",
    "    \"Customer Count vs Upgrades by Distance\\n(Side-by-side Comparison)\",\n",
    "    fontweight=\"bold\",\n",
    "    pad=20,\n",
    ")\n",
    "ax3.set_ylabel(\"Number of Customers\", fontweight=\"bold\")\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(dist_analysis.index, rotation=45)\n",
    "ax3.legend(framealpha=0.9)\n",
    "ax3.grid(axis=\"y\", alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# 4. Heatmap showing upgrade patterns\n",
    "pivot_data = train_data.groupby([\"dist_label\", \"UPD\"]).size().unstack(fill_value=0)\n",
    "pivot_data = pivot_data.reindex([\"< 10 min\", \"10-20 min\", \"21-30 min\", \"> 30 min\"])\n",
    "pivot_pct = pivot_data.div(pivot_data.sum(axis=1), axis=0) * 100\n",
    "\n",
    "sns.heatmap(\n",
    "    pivot_pct,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=\"RdYlGn\",\n",
    "    center=50,\n",
    "    ax=ax4,\n",
    "    cbar_kws={\"label\": \"Percentage\"},\n",
    ")\n",
    "ax4.set_title(\n",
    "    \"Upgrade Behavior Heatmap\\n(Percentage Distribution)\", fontweight=\"bold\", pad=20\n",
    ")\n",
    "ax4.set_xlabel(\"Upgrade Decision\", fontweight=\"bold\")\n",
    "ax4.set_ylabel(\"Distance from Zoo\", fontweight=\"bold\")\n",
    "ax4.set_xticklabels([\"No Upgrade\", \"Upgrade\"], rotation=0)\n",
    "ax4.set_yticklabels(ax4.get_yticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602094b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Generate insights summary for presentation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY INSIGHTS FROM DISTANCE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find the distance category with highest and lowest upgrade rates\n",
    "max_upgrade_dist = dist_analysis[\"Upgrade_Rate\"].idxmax()\n",
    "min_upgrade_dist = dist_analysis[\"Upgrade_Rate\"].idxmin()\n",
    "max_rate = dist_analysis.loc[max_upgrade_dist, \"Upgrade_Rate\"] * 100\n",
    "min_rate = dist_analysis.loc[min_upgrade_dist, \"Upgrade_Rate\"] * 100\n",
    "\n",
    "print(\"\\nüìç DISTANCE IMPACT ON UPGRADES:\")\n",
    "print(f\"   ‚Ä¢ Highest upgrade rate: {max_upgrade_dist} ({max_rate:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Lowest upgrade rate:  {min_upgrade_dist} ({min_rate:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Difference: {max_rate - min_rate:.1f} percentage points\")\n",
    "\n",
    "# Calculate statistical significance (Chi-square test)\n",
    "chi2, p_value, dof, expected = chi2_contingency(dist_crosstab)\n",
    "\n",
    "print(\"\\nüìä STATISTICAL SIGNIFICANCE:\")\n",
    "print(f\"   ‚Ä¢ Chi-square statistic: {chi2:.3f}\")\n",
    "print(f\"   ‚Ä¢ P-value: {p_value:.4f}\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ Significance: {'Significant' if p_value < 0.05 else 'Not significant'} at Œ±=0.05\"\n",
    ")\n",
    "\n",
    "# Business implications\n",
    "print(\"\\nüí° BUSINESS IMPLICATIONS:\")\n",
    "if max_rate > 50:\n",
    "    print(f\"   ‚Ä¢ Customers living {max_upgrade_dist} show higher upgrade propensity\")\n",
    "    print(\"   ‚Ä¢ Consider targeted marketing for this distance segment\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ Distance appears to negatively impact upgrade likelihood\")\n",
    "    print(\"   ‚Ä¢ Focus on improving value proposition for distant customers\")\n",
    "\n",
    "print(f\"   ‚Ä¢ Overall upgrade rate: {train_data['UPD'].mean() * 100:.1f}%\")\n",
    "print(\"   ‚Ä¢ Distance-based segmentation may be valuable for strategy\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6fa953",
   "metadata": {},
   "source": [
    "#### Visit Frequency vs Upgrade Behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946e5c2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create visit frequency labels for better visualization\n",
    "visit_labels = {\n",
    "    1: \"‚â§ 2 visits\",\n",
    "    2: \"3-4 visits\",\n",
    "    3: \"5-6 visits\",\n",
    "    4: \"7-8 visits\",\n",
    "    5: \"> 8 visits\",\n",
    "}\n",
    "train_data[\"tvis_label\"] = train_data[\"tvis\"].map(visit_labels)\n",
    "\n",
    "# Calculate upgrade rates by visit frequency\n",
    "visit_analysis = (\n",
    "    train_data.groupby(\"tvis_label\").agg({\"UPD\": [\"count\", \"sum\", \"mean\"]}).round(3)\n",
    ")\n",
    "visit_analysis.columns = [\"Total_Customers\", \"Upgrades\", \"Upgrade_Rate\"]\n",
    "visit_analysis = visit_analysis.reindex(\n",
    "    [\"‚â§ 2 visits\", \"3-4 visits\", \"5-6 visits\", \"7-8 visits\", \"> 8 visits\"]\n",
    ")\n",
    "\n",
    "# Create a beautiful visualization for visit frequency vs upgrades\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# 1. Upgrade rate by visit frequency\n",
    "upgrade_rates_visits = visit_analysis[\"Upgrade_Rate\"] * 100\n",
    "colors_visits = [\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\", \"#96CEB4\", \"#FFA07A\"]\n",
    "\n",
    "bars = ax1.bar(\n",
    "    range(len(upgrade_rates_visits)),\n",
    "    upgrade_rates_visits.values,\n",
    "    color=colors_visits,\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, (bar, rate) in enumerate(zip(bars, upgrade_rates_visits.values)):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 1,\n",
    "        f\"{rate:.1f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "\n",
    "ax1.set_title(\n",
    "    \"Upgrade Rate by Visit Frequency\\n(Higher engagement = Higher upgrades?)\",\n",
    "    fontweight=\"bold\",\n",
    "    pad=20,\n",
    ")\n",
    "ax1.set_ylabel(\"Upgrade Rate (%)\", fontweight=\"bold\")\n",
    "ax1.set_xticks(range(len(upgrade_rates_visits)))\n",
    "ax1.set_xticklabels(upgrade_rates_visits.index, rotation=45, ha=\"right\")\n",
    "ax1.set_ylim(0, max(upgrade_rates_visits.values) * 1.2)\n",
    "ax1.grid(axis=\"y\", alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# Add average line\n",
    "avg_rate = train_data[\"UPD\"].mean() * 100\n",
    "ax1.axhline(y=avg_rate, color=\"red\", linestyle=\"--\", alpha=0.7, linewidth=2)\n",
    "ax1.text(\n",
    "    0.02,\n",
    "    avg_rate + 2,\n",
    "    f\"Overall Average: {avg_rate:.1f}%\",\n",
    "    transform=ax1.get_yaxis_transform(),\n",
    "    fontsize=10,\n",
    "    color=\"red\",\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# 2. Customer distribution by visit frequency\n",
    "visit_counts = train_data[\"tvis_label\"].value_counts()\n",
    "visit_counts = visit_counts.reindex(\n",
    "    [\"‚â§ 2 visits\", \"3-4 visits\", \"5-6 visits\", \"7-8 visits\", \"> 8 visits\"]\n",
    ")\n",
    "\n",
    "bars = ax2.bar(\n",
    "    range(len(visit_counts)),\n",
    "    visit_counts.values,\n",
    "    color=colors_visits,\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, (bar, count) in enumerate(zip(bars, visit_counts.values)):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 3,\n",
    "        f\"{count}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=11,\n",
    "    )\n",
    "    # Add percentage of total\n",
    "    pct = count / len(train_data) * 100\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height / 2,\n",
    "        f\"{pct:.1f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        color=\"white\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "ax2.set_title(\n",
    "    \"Customer Distribution by Visit Frequency\\n(Engagement Patterns)\",\n",
    "    fontweight=\"bold\",\n",
    "    pad=20,\n",
    ")\n",
    "ax2.set_ylabel(\"Number of Customers\", fontweight=\"bold\")\n",
    "ax2.set_xticks(range(len(visit_counts)))\n",
    "ax2.set_xticklabels(visit_counts.index, rotation=45, ha=\"right\")\n",
    "ax2.set_ylim(0, max(visit_counts.values) * 1.15)\n",
    "ax2.grid(axis=\"y\", alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nVisit Frequency vs Upgrade Analysis:\")\n",
    "print(\"=\" * 55)\n",
    "for visit, row in visit_analysis.iterrows():\n",
    "    print(\n",
    "        f\"{visit:12s}: {row['Total_Customers']:3.0f} customers, \"\n",
    "        f\"{row['Upgrades']:3.0f} upgrades ({row['Upgrade_Rate'] * 100:5.1f}%)\"\n",
    "    )\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232bcfa",
   "metadata": {},
   "source": [
    "### Data Preprocessing & Feature Engineering\n",
    "\n",
    "#### Objective: Prepare data for machine learning models with appropriate encoding and scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f4843",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Check current data shape and missing values\n",
    "print(\"Data Preprocessing Setup\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"Missing values in training: {train_data.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test: {test_data.isnull().sum().sum()}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb8e21d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Feature categorization based on business logic and statistical properties\n",
    "print(\"\\nFeature Categorization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Features to exclude (ID variables)\n",
    "id_features = [\"NID\"]\n",
    "\n",
    "# Target variable\n",
    "target = \"UPD\"\n",
    "\n",
    "# Categorical features (non-linear relationship) - need dummy encoding\n",
    "categorical_nonlinear = [\n",
    "    \"gender\",  # 1=Male, 2=Female (nominal)\n",
    "    \"mstat\",  # 1=Married, 2=Single, 3=Divorced, 4=Widow (nominal)\n",
    "    \"educ\",  # 1=Some college, 2=College, 3=Graduate school (treated as nominal per user)\n",
    "    \"educnew\",  # Recoded education variable (nominal)\n",
    "    \"age_rec\",  # 1=18-34, 2=35-44, 3=45-54, 4=54+ (treated as nominal per user)\n",
    "]\n",
    "\n",
    "# Ordinal features with linear relationship - use numeric + normalize\n",
    "ordinal_linear = [\n",
    "    \"dist\",  # 1=<10min, 2=10-20min, 3=21-30min, 4=>30min (clear distance progression)\n",
    "    \"tvis\",  # 1=‚â§2, 2=3-4, 3=5-6, 4=7-8, 5=>8 visits (treated as nominal per user)\n",
    "]\n",
    "\n",
    "# Already standardized perception features (mean‚âà0, std‚âà1)\n",
    "standardized_features = [\n",
    "    \"benefits\",\n",
    "    \"costs\",\n",
    "    \"value\",\n",
    "    \"identity\",\n",
    "    \"know\",\n",
    "    \"sat\",\n",
    "    \"fle\",\n",
    "    \"trustfor\",\n",
    "]\n",
    "\n",
    "# Other numeric features that need scaling\n",
    "numeric_features = [\n",
    "    \"age\",  # Age in categories (1-5 scale)\n",
    "    \"size\",  # Household size (1-6)\n",
    "    \"child1\",  # Number of children/grandchildren (0-6)\n",
    "]\n",
    "\n",
    "print(f\"Categorical (dummy encode): {categorical_nonlinear}\")\n",
    "print(f\"Ordinal linear (normalize): {ordinal_linear}\")\n",
    "print(f\"Already standardized: {standardized_features}\")\n",
    "print(f\"Numeric (need scaling): {numeric_features}\")\n",
    "print(f\"ID features (exclude): {id_features}\")\n",
    "print(f\"Target variable: {target}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba60ff0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create comprehensive preprocessing pipeline\n",
    "print(\"\\nBuilding Preprocessing Pipeline:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create preprocessing steps for different feature types\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # One-hot encode categorical features (drop='first' to avoid multicollinearity)\n",
    "        (\n",
    "            \"categorical\",\n",
    "            OneHotEncoder(drop=\"first\", sparse_output=False),\n",
    "            categorical_nonlinear,\n",
    "        ),\n",
    "        # Standardize ordinal features with linear relationship\n",
    "        (\"ordinal\", StandardScaler(), ordinal_linear),\n",
    "        # Keep already standardized features as-is\n",
    "        (\"standardized\", \"passthrough\", standardized_features),\n",
    "        # Standardize other numeric features\n",
    "        (\"numeric\", StandardScaler(), numeric_features),\n",
    "    ],\n",
    "    remainder=\"drop\",  # Drop any remaining features (like NID)\n",
    ")\n",
    "\n",
    "print(\"Pipeline Components:\")\n",
    "print(\"‚Ä¢ OneHotEncoder: Categorical features ‚Üí Binary dummy variables\")\n",
    "print(\"‚Ä¢ StandardScaler: Ordinal/Numeric features ‚Üí Mean=0, Std=1\")\n",
    "print(\"‚Ä¢ PassThrough: Pre-standardized features ‚Üí Unchanged\")\n",
    "print(\"‚Ä¢ Remainder: ID features ‚Üí Dropped\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed40427",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Prepare data for preprocessing\n",
    "print(\"\\nPreparing Data for Preprocessing:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Separate features and target for training data\n",
    "X_train_raw = train_data.drop(columns=[target])\n",
    "y_train = train_data[target]\n",
    "\n",
    "# Separate features for test data (test data has target too, but we'll use it for final evaluation)\n",
    "X_test_raw = test_data.drop(columns=[target])\n",
    "y_test = test_data[target]\n",
    "\n",
    "print(f\"Training features shape: {X_train_raw.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Test features shape: {X_test_raw.shape}\")\n",
    "print(f\"Test target shape: {y_test.shape}\")\n",
    "\n",
    "# Show feature names before preprocessing\n",
    "print(f\"\\nOriginal features ({len(X_train_raw.columns)}):\")\n",
    "print(list(X_train_raw.columns))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8f24d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing pipeline\n",
    "print(\"\\nApplying Preprocessing Pipeline:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fit preprocessor on training data only (prevents data leakage)\n",
    "print(\"Fitting preprocessor on training data...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train_raw)\n",
    "\n",
    "# Transform test data using fitted preprocessor\n",
    "print(\"Transforming test data...\")\n",
    "X_test_processed = preprocessor.transform(X_test_raw)\n",
    "\n",
    "print(f\"Processed training shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed test shape: {X_test_processed.shape}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac04c03",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "print(\"\\nFeature Names After Preprocessing:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get feature names from the fitted preprocessor\n",
    "feature_names = []\n",
    "\n",
    "# Get names from each transformer\n",
    "categorical_names = list(\n",
    "    preprocessor.named_transformers_[\"categorical\"].get_feature_names_out(\n",
    "        categorical_nonlinear\n",
    "    )\n",
    ")\n",
    "ordinal_names = [f\"ordinal__{col}\" for col in ordinal_linear]\n",
    "standardized_names = [f\"standardized__{col}\" for col in standardized_features]\n",
    "numeric_names = [f\"numeric__{col}\" for col in numeric_features]\n",
    "\n",
    "# Combine all feature names\n",
    "feature_names = categorical_names + ordinal_names + standardized_names + numeric_names\n",
    "\n",
    "print(f\"Total features after preprocessing: {len(feature_names)}\")\n",
    "print(f\"Feature expansion: {X_train_raw.shape[1]} ‚Üí {len(feature_names)} features\")\n",
    "print(\"\\nFeature breakdown:\")\n",
    "print(f\"‚Ä¢ Categorical (dummy): {len(categorical_names)} features\")\n",
    "print(f\"‚Ä¢ Ordinal (scaled): {len(ordinal_names)} features\")\n",
    "print(f\"‚Ä¢ Standardized (passthrough): {len(standardized_names)} features\")\n",
    "print(f\"‚Ä¢ Numeric (scaled): {len(numeric_names)} features\")\n",
    "\n",
    "# Show first few categorical feature names (dummy variables)\n",
    "print(f\"\\nSample categorical features (first 10):\")\n",
    "print(categorical_names[:10])\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc297afe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create validation split from training data\n",
    "print(\"\\nCreating Validation Split:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Split training data into train/validation (80/20 split, stratified)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_processed, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training split: {X_train_split.shape}\")\n",
    "print(f\"Validation split: {X_val_split.shape}\")\n",
    "print(f\"Training target distribution: {np.bincount(y_train_split)}\")\n",
    "print(f\"Validation target distribution: {np.bincount(y_val_split)}\")\n",
    "print(f\"Training upgrade rate: {y_train_split.mean():.3f}\")\n",
    "print(f\"Validation upgrade rate: {y_val_split.mean():.3f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a318da2",
   "metadata": {},
   "source": [
    "### Feature Engineering & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb23ad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create DataFrame for easier manipulation and feature engineering\n",
    "print(\"\\nFeature Engineering Setup:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Convert to DataFrame for easier feature engineering\n",
    "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "X_val_df = pd.DataFrame(X_val_split, columns=feature_names)\n",
    "X_test_df = pd.DataFrame(X_test_processed, columns=feature_names)\n",
    "\n",
    "print(f\"Training DataFrame shape: {X_train_df.shape}\")\n",
    "print(f\"Validation DataFrame shape: {X_val_df.shape}\")\n",
    "print(f\"Test DataFrame shape: {X_test_df.shape}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad27d41",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Correlation analysis and feature relationships\n",
    "print(\"\\nCorrelation Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = X_train_df.corr()\n",
    "\n",
    "# Find highly correlated features (threshold = 0.8)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append(\n",
    "                (\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j],\n",
    "                    correlation_matrix.iloc[i, j],\n",
    "                )\n",
    "            )\n",
    "\n",
    "print(f\"Found {len(high_corr_pairs)} highly correlated feature pairs (|r| > 0.8):\")\n",
    "for feat1, feat2, corr in high_corr_pairs[:10]:  # Show first 10\n",
    "    print(f\"  {feat1} ‚Üî {feat2}: {corr:.3f}\")\n",
    "\n",
    "if len(high_corr_pairs) > 10:\n",
    "    print(f\"  ... and {len(high_corr_pairs) - 10} more pairs\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831f256",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Feature correlation with target variable\n",
    "print(\"\\nFeature-Target Correlations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate correlations with target\n",
    "target_correlations = []\n",
    "for feature in feature_names:\n",
    "    corr = np.corrcoef(X_train_df[feature], y_train_split)[0, 1]\n",
    "    target_correlations.append((feature, corr))\n",
    "\n",
    "# Sort by absolute correlation\n",
    "target_correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"Top 15 features most correlated with upgrade decision:\")\n",
    "for i, (feature, corr) in enumerate(target_correlations[:15], 1):\n",
    "    print(f\"{i:2d}. {feature:<40}: {corr:6.3f}\")\n",
    "\n",
    "print(\"\\nBottom 5 features least correlated with upgrade decision:\")\n",
    "for i, (feature, corr) in enumerate(\n",
    "    target_correlations[-5:], len(target_correlations) - 4\n",
    "):\n",
    "    print(f\"{i:2d}. {feature:<40}: {corr:6.3f}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee577f65",
   "metadata": {},
   "source": [
    "### Machine Learning Model Development\n",
    "\n",
    "#### Implementing 5 models: Logistic Regression, SVM, Naive Bayes, Random Forest, GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3dc9d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import machine learning libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "import time\n",
    "\n",
    "print(\"Machine Learning Model Setup\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa7394",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define models with initial parameters\n",
    "print(\"\\nInitializing Models:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        random_state=42, max_iter=1000, penalty=\"l2\"\n",
    "    ),\n",
    "    \"SVM\": SVC(\n",
    "        random_state=42,\n",
    "        probability=True,  # Enable probability estimates for ROC-AUC\n",
    "        kernel=\"rbf\",\n",
    "    ),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"‚úì {name}: {type(model).__name__}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38913aa7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cross-validation evaluation\n",
    "print(\"\\nCross-Validation Evaluation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Setup stratified k-fold cross-validation\n",
    "cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "cv_results = {}\n",
    "training_times = {}\n",
    "\n",
    "print(\"Performing 5-fold cross-validation for each model...\")\n",
    "print(\"\\nModel Performance (CV Mean ¬± Std):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    # Time the training\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_split, y_train_split, cv=cv_folds, scoring=\"roc_auc\", n_jobs=-1\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # Store results\n",
    "    cv_results[name] = cv_scores\n",
    "    training_times[name] = training_time\n",
    "\n",
    "    # Print results\n",
    "    mean_score = cv_scores.mean()\n",
    "    std_score = cv_scores.std()\n",
    "    print(\n",
    "        f\"{name:<20}: {mean_score:.4f} ¬± {std_score:.4f} (Time: {training_time:.2f}s)\"\n",
    "    )\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"Metric: ROC-AUC (Higher is better)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd4c48",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Train models on full training split and evaluate on validation\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Store validation results\n",
    "val_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"Training models on full training split and evaluating on validation set...\")\n",
    "print(\"\\nValidation Performance:\")\n",
    "print(\"-\" * 80)\n",
    "print(\n",
    "    f\"{'Model':<20} {'ROC-AUC':<8} {'Accuracy':<8} {'Precision':<10} {'Recall':<8} {'F1-Score':<8}\"\n",
    ")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model on training split\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "    trained_models[name] = model\n",
    "\n",
    "    # Predict on validation set\n",
    "    val_pred = model.predict(X_val_split)\n",
    "    val_pred_proba = model.predict_proba(X_val_split)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    val_auc = roc_auc_score(y_val_split, val_pred_proba)\n",
    "    val_accuracy = (val_pred == y_val_split).mean()\n",
    "\n",
    "    # Get precision, recall, f1 from classification report\n",
    "    report = classification_report(y_val_split, val_pred, output_dict=True)\n",
    "    val_precision = report[\"1\"][\"precision\"]\n",
    "    val_recall = report[\"1\"][\"recall\"]\n",
    "    val_f1 = report[\"1\"][\"f1-score\"]\n",
    "\n",
    "    # Store results\n",
    "    val_results[name] = {\n",
    "        \"auc\": val_auc,\n",
    "        \"accuracy\": val_accuracy,\n",
    "        \"precision\": val_precision,\n",
    "        \"recall\": val_recall,\n",
    "        \"f1\": val_f1,\n",
    "        \"predictions\": val_pred,\n",
    "        \"probabilities\": val_pred_proba,\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(\n",
    "        f\"{name:<20} {val_auc:<8.4f} {val_accuracy:<8.4f} {val_precision:<10.4f} {val_recall:<8.4f} {val_f1:<8.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de25a92",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Model ranking and selection\n",
    "print(\"\\nModel Ranking and Selection:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Rank models by ROC-AUC on validation set\n",
    "model_ranking = sorted(val_results.items(), key=lambda x: x[1][\"auc\"], reverse=True)\n",
    "\n",
    "print(\"Model Performance Ranking (by ROC-AUC):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Rank':<5} {'Model':<20} {'ROC-AUC':<10} {'CV Score':<12} {'Robustness'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, (name, results) in enumerate(model_ranking, 1):\n",
    "    cv_mean = cv_results[name].mean()\n",
    "    cv_std = cv_results[name].std()\n",
    "    robustness = \"High\" if cv_std < 0.02 else \"Medium\" if cv_std < 0.05 else \"Low\"\n",
    "\n",
    "    print(\n",
    "        f\"{i:<5} {name:<20} {results['auc']:<10.4f} {cv_mean:.4f}¬±{cv_std:.3f} {robustness}\"\n",
    "    )\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Select best model\n",
    "best_model_name = model_ranking[0][0]\n",
    "best_model = trained_models[best_model_name]\n",
    "best_results = val_results[best_model_name]\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ ROC-AUC: {best_results['auc']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {best_results['accuracy']:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score: {best_results['f1']:.4f}\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ Cross-validation: {cv_results[best_model_name].mean():.4f} ¬± {cv_results[best_model_name].std():.3f}\"\n",
    ")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bdf826",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Feature importance analysis for interpretable models\n",
    "print(\"\\nFeature Importance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extract feature importance from different models\n",
    "importance_results = {}\n",
    "\n",
    "# 1. Logistic Regression - Coefficients\n",
    "if \"Logistic Regression\" in trained_models:\n",
    "    lr_model = trained_models[\"Logistic Regression\"]\n",
    "    lr_coef = lr_model.coef_[0]\n",
    "    importance_results[\"Logistic Regression\"] = list(zip(feature_names, lr_coef))\n",
    "\n",
    "# 2. Random Forest - Feature Importance\n",
    "if \"Random Forest\" in trained_models:\n",
    "    rf_model = trained_models[\"Random Forest\"]\n",
    "    rf_importance = rf_model.feature_importances_\n",
    "    importance_results[\"Random Forest\"] = list(zip(feature_names, rf_importance))\n",
    "\n",
    "# 3. Gradient Boosting - Feature Importance\n",
    "if \"Gradient Boosting\" in trained_models:\n",
    "    gb_model = trained_models[\"Gradient Boosting\"]\n",
    "    gb_importance = gb_model.feature_importances_\n",
    "    importance_results[\"Gradient Boosting\"] = list(zip(feature_names, gb_importance))\n",
    "\n",
    "# Display feature importance for best model\n",
    "if best_model_name in importance_results:\n",
    "    print(f\"\\nTop 15 Most Important Features ({best_model_name}):\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    best_importance = importance_results[best_model_name]\n",
    "    if best_model_name == \"Logistic Regression\":\n",
    "        # Sort by absolute coefficient value\n",
    "        best_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        print(f\"{'Feature':<40} {'Coefficient':<15} {'Abs Value':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        for i, (feature, coef) in enumerate(best_importance[:15], 1):\n",
    "            print(f\"{i:2d}. {feature:<35} {coef:8.4f} {abs(coef):8.4f}\")\n",
    "    else:\n",
    "        # Sort by importance value\n",
    "        best_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"{'Feature':<40} {'Importance':<15}\")\n",
    "        print(\"-\" * 70)\n",
    "        for i, (feature, importance) in enumerate(best_importance[:15], 1):\n",
    "            print(f\"{i:2d}. {feature:<35} {importance:8.4f}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1638c04",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Final test set evaluation\n",
    "print(\"\\nFinal Test Set Evaluation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Retrain best model on full training data (training + validation)\n",
    "print(f\"Retraining {best_model_name} on full training data...\")\n",
    "final_model = models[best_model_name]\n",
    "final_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "test_pred = final_model.predict(X_test_processed)\n",
    "test_pred_proba = final_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Calculate test metrics\n",
    "test_auc = roc_auc_score(y_test, test_pred_proba)\n",
    "test_accuracy = (test_pred == y_test).mean()\n",
    "\n",
    "# Get detailed metrics\n",
    "test_report = classification_report(y_test, test_pred, output_dict=True)\n",
    "test_precision = test_report[\"1\"][\"precision\"]\n",
    "test_recall = test_report[\"1\"][\"recall\"]\n",
    "test_f1 = test_report[\"1\"][\"f1-score\"]\n",
    "\n",
    "print(\"\\nFinal Model Performance on Test Set:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"ROC-AUC:   {test_auc:.4f}\")\n",
    "print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall:    {test_recall:.4f}\")\n",
    "print(f\"F1-Score:  {test_f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "test_cm = confusion_matrix(y_test, test_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted\")\n",
    "print(\"Actual    No Upgrade  Upgrade\")\n",
    "print(f\"No Upgrade    {test_cm[0, 0]:4d}     {test_cm[0, 1]:4d}\")\n",
    "print(f\"Upgrade       {test_cm[1, 0]:4d}     {test_cm[1, 1]:4d}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713150da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business insights and recommendations\n",
    "print(\"\\nBusiness Insights & Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Best performing model: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Test set accuracy: {test_accuracy:.1%}\")\n",
    "print(f\"   ‚Ä¢ Test set ROC-AUC: {test_auc:.3f}\")\n",
    "print(f\"   ‚Ä¢ Model can identify {test_recall:.1%} of customers who will upgrade\")\n",
    "print(f\"   ‚Ä¢ {test_precision:.1%} of predicted upgrades are correct\")\n",
    "\n",
    "# Calculate business impact\n",
    "total_customers = len(test_data)\n",
    "actual_upgrades = y_test.sum()\n",
    "predicted_upgrades = test_pred.sum()\n",
    "correctly_identified = (test_pred & y_test).sum()\n",
    "\n",
    "print(\"\\nüíº BUSINESS IMPACT:\")\n",
    "print(f\"   ‚Ä¢ Total test customers: {total_customers}\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ Actual upgrades: {actual_upgrades} ({actual_upgrades / total_customers:.1%})\"\n",
    ")\n",
    "print(f\"   ‚Ä¢ Predicted upgrades: {predicted_upgrades}\")\n",
    "print(f\"   ‚Ä¢ Correctly identified upgrades: {correctly_identified}\")\n",
    "print(\"   ‚Ä¢ Potential revenue impact: High (targeted marketing efficiency)\")\n",
    "\n",
    "print(\"\\nüéØ KEY RECOMMENDATIONS:\")\n",
    "if best_model_name in importance_results:\n",
    "    top_features = importance_results[best_model_name]\n",
    "    if best_model_name == \"Logistic Regression\":\n",
    "        top_features.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    else:\n",
    "        top_features.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"   ‚Ä¢ Focus on top predictive features:\")\n",
    "    for i, (feature, _) in enumerate(top_features[:3], 1):\n",
    "        clean_feature = (\n",
    "            feature.replace(\"standardized__\", \"\")\n",
    "            .replace(\"categorical__\", \"\")\n",
    "            .replace(\"numeric__\", \"\")\n",
    "            .replace(\"ordinal__\", \"\")\n",
    "        )\n",
    "        print(f\"     {i}. {clean_feature}\")\n",
    "\n",
    "print(\"   ‚Ä¢ Implement targeted retention strategies\")\n",
    "print(\"   ‚Ä¢ Use model for customer segmentation\")\n",
    "print(\"   ‚Ä¢ Monitor model performance quarterly\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
